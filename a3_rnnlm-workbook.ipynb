{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network Language Model\n",
    "\n",
    "This is the \"working notebook\", with skeleton code to load and train your model, as well as run unit tests. See [rnnlm-instructions.ipynb](rnnlm-instructions.ipynb) for the main writeup.\n",
    "\n",
    "Run the cell below to import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nconidas/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'rnnlm_test' from '/home/nconidas/w266/assignment/a3/lstm/rnnlm_test.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz\n",
    "\n",
    "# Your code\n",
    "import rnnlm; reload(rnnlm)\n",
    "import rnnlm_test; reload(rnnlm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) RNNLM Inputs and Parameters  \n",
    "\n",
    "### Questions for Part (a)\n",
    "You should use big-O notation when appropriate (i.e. computing $\\exp(\\mathbf{v})$ for a vector $\\mathbf{v}$ of length $n$ is $O(n)$ operations).  Assume for problems a(1-5) that:   \n",
    "> -- Cell is one layer,  \n",
    "> -- the embedding feature length and hidden-layer feature lengths are both H, and   \n",
    "> -- ignore at the moment batch and max_time dimensions.  \n",
    "\n",
    "1. Let $\\text{CellFunc}$ be a simple RNN cell (see async Section 5.8). Write the cell equation in terms of nonlinearities and matrix multiplication. How many parameters (matrix or vector elements) are there for this cell, in terms of `V` and `H`?\n",
    "<p>\n",
    "2. How many parameters are in the embedding layer? In the output layer? (By parameters, we mean total number of matrix elements across all train-able tensors. A $m \\times n$ matrix has $mn$ elements.)\n",
    "<p>\n",
    "3. How many calculations (floating point operations) are required to compute $\\hat{P}(w^{(i+1)})$ for a given *single* target word $w^{(i+1)}$, assuming $w^{(i)}$ given and $h^{(i-1)}$ already computed? How about for *all* target words?\n",
    "<p>\n",
    "4. How does your answer to 3. change if we approximate $\\hat{P}(w^{(i+1)})$ with a sampled softmax with $k$ samples? How about if we use a hierarchical softmax? (*Recall that hierarchical softmax makes a series of left/right decisions using a binary classifier $P_s(\\text{right}) = \\sigma(u_s \\cdot o^{(i)} + b_s)$ at each split $s$ in the tree.*)\n",
    "<p>\n",
    "5. If you have an LSTM with $H = 200$ and use sampled softmax with $k = 100$, what part of the network takes up the most computation time during training? (*Choose \"embedding layer\", \"recurrent layer\", or \"output layer\"*.)\n",
    "\n",
    "Note: for $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times l}$, computing the matrix product $AB$ takes $O(mnl)$ time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. tanh(WM+B) weight matrix w = [2H, H], M = [H], b = [1,H]\n",
    "\n",
    "2. Embedding Layer Dim = [H], Output Layer Dim = [H,V]\n",
    "\n",
    "3. \n",
    "\n",
    "  * #### One Target Word \n",
    "      * Recurrent Layer: [1,2H] x [V,H] = O(2H^2) \n",
    "      * Output Layer: [1,H] x [H,1] = O(H) Therefore this takes O(H^2) time\n",
    "  * #### All Target Words \n",
    "      * Recurrent Layer: [1,2H] * [2H,H] = O(2H^2)\n",
    "      * Output Layer: [1,H]*[H,V] = O(H*V) Therefore this takes O(HV) time\n",
    "      *  V may be orders of magnitude large than H so the differnce can be large.\n",
    "\n",
    "4. \n",
    "  * Instead of calculating a soft max for every word in V sampled soft max will only do it for k words therefore it will run in O(Hk) time\n",
    "  \n",
    "  * Hierarchical Soft Max will run in O(Log(HV)) time \n",
    " \n",
    "5. \n",
    "  * #### Recurrent Layer \n",
    "      * [1,400] x [400, 200] = O(80,000) \n",
    "  * #### Output Layer\n",
    "      * [1,200] x [200, 100] = O(20,000)\n",
    "      \n",
    "  * The Recurrent Layer should take the longest \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Implementing the RNNLM\n",
    "\n",
    "### Aside: Shapes Review\n",
    "\n",
    "Before we start, let's review our understanding of the shapes involved in this assignment and how they manifest themselves in the TF API.\n",
    "\n",
    "As in the [instructions](rnnlm-instructions.ipynb) notebook, $w$ is a matrix of wordids with shape batch_size x max_time.  Passing this through the embedding layer, we retrieve the word embedding for each, resulting in $x$ having shape batch_size x max_time x embedding_dim.  I find it useful to draw this out on a piece of paper.  When you do, you should end up with a rectangular prism with batch_size height, max_time width and embedding_dim depth.  Many tensors in this assignment share this shape (e.g. $o$, the output from the LSTM, which represents the hidden layer going into the softmax to make a prediction at every time step in every batch).\n",
    "\n",
    "![Three Dimensional Shape](common_shape.png)\n",
    "\n",
    "Since batch size and sentence length are only resolved when we run the graph, we construct the placeholder using \"None\" in the dimensions we don't know.  The .shape property renders these as ?s.  This should be familiar to you from batch size handling in earlier assignments, only now there are two dimensions of variable length.\n",
    "\n",
    "See the next cell for a concrete example (though in practice, we'd use a TensorFlow variable that we can train for the embeddings rather than a static array).  Notice how the shape of x_val matches the shape described earlier in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordid placeholder shape: (?, ?)\n",
      "x shape: (?, ?, 3)\n",
      "Embeddings shape: (2, 4, 3)\n",
      "Embeddings value:\n",
      " [[[2 2 2]\n",
      "  [3 3 3]\n",
      "  [2 2 2]\n",
      "  [3 3 3]]\n",
      "\n",
      " [[1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "wordid_ph = tf.placeholder(tf.int32, shape=[None, None])\n",
    "embedding_matrix = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\n",
    "x = tf.nn.embedding_lookup(embedding_matrix, wordid_ph)\n",
    "\n",
    "print('wordid placeholder shape:', wordid_ph.shape)\n",
    "print('x shape:', x.shape)\n",
    "\n",
    "sess = tf.Session()\n",
    "# Two sentences, each with four words.\n",
    "wordids = [[1, 2, 1, 2], [0, 0, 0, 0]]\n",
    "x_val = sess.run(x, feed_dict={wordid_ph: wordids})\n",
    "print('Embeddings shape:', x_val.shape)  # 2 sentences, 4 words, \n",
    "print('Embeddings value:\\n', x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implmenting the RNNLM\n",
    "\n",
    "In order to better manage the model parameters, we'll implement our RNNLM in the `RNNLM` class in `rnnlm.py`. We've given you a skeleton of starter code for this, but the bulk of the implementation is left to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, 10000)\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm)\n",
    "\n",
    "TF_GRAPHDIR = \"/tmp/w266/a3_graph\"\n",
    "\n",
    "# Clear old log directory.\n",
    "shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(V=10000, H=200, num_layers=2)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above will load your implementation, construct the graph, and write a logdir for TensorBoard. You can bring up TensorBoard with:\n",
    "```\n",
    "cd assignment/a3\n",
    "tensorboard --logdir /tmp/w266/a3_graph --port 6006\n",
    "```\n",
    "As usual, check http://localhost:6006/ and visit the \"Graphs\" tab to inspect your implementation. Remember, judicious use of `tf.name_scope()` and/or `tf.variable_scope()` will greatly improve the visualization, and make code easier to debug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've provided a few unit tests below to verify some *very* basic properties of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_shapes_embed (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_output (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_recurrent (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_train (rnnlm_test.TestRNNLMTrain) ... ok\n",
      "test_shapes_sample (rnnlm_test.TestRNNLMSampler) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_1:0\", shape=(?, ?, 1), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 2.025s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm); reload(rnnlm_test)\n",
    "utils.run_tests(rnnlm_test, [\"TestRNNLMCore\", \"TestRNNLMTrain\", \"TestRNNLMSampler\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the error messages are intentionally somewhat spare, and that passing tests are no guarantee of model correctness! Your best chance of success is through careful coding and understanding of how the model works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Training your RNNLM (5 points)\n",
    "\n",
    "We'll give you data loader functions in **`utils.py`**. They work similarly to the loaders in the Week 5 notebook.\n",
    "\n",
    "Particularly, `utils.rnnlm_batch_generator` will return an iterator that yields minibatches in the correct format. Batches will be of size `[batch_size, max_time]`, and consecutive batches will line up along rows so that the final state $h^{\\text{final}}$ of one batch can be used as the initial state $h^{\\text{init}}$ for the next.\n",
    "\n",
    "For example, using a toy corpus:  \n",
    "*(Ignore the ugly formatter code.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Input words w:</h3><table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "      <th>w_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>little</td>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white</td>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Target words y:</h3><table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "      <th>y_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toy_corpus = \"<s> Mary had a little lamb . <s> The lamb was white as snow . <s>\"\n",
    "toy_corpus = np.array(toy_corpus.split())\n",
    "\n",
    "html = \"<h3>Input words w:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "bi = utils.rnnlm_batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    cols = [\"w_%d\" % d for d in range(w.shape[1])]\n",
    "    html += \"<td>{:s}</td>\".format(utils.render_matrix(w, cols=cols, dtype=object))\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))\n",
    "\n",
    "html = \"<h3>Target words y:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "bi = utils.rnnlm_batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    cols = [\"y_%d\" % d for d in range(y.shape[1])]\n",
    "    html += \"<td>{:s}</td>\".format(utils.render_matrix(y, cols=cols, dtype=object))\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data we feed to our model will be word indices, but the shape will be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement the `run_epoch` function\n",
    "We've given you some starter code for logging progress; fill this in with actual call(s) to `session.run` with the appropriate arguments to run a training step. \n",
    "\n",
    "Be sure to handle the initial state properly at the beginning of an epoch, and remember to carry over the final state from each batch and use it as the initial state for the next.\n",
    "\n",
    "**Note:** we provide a `train=True` flag to enable train mode. If `train=False`, this function can also be used for scoring the dataset - see `score_dataset()` below.\n",
    "\n",
    "#### Questions\n",
    "\n",
    "1.  Explain what this function does.  Be sure to include the role of `batch_iterator` and what's going on with `h` in the inner loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        feed_dict = {\n",
    "            lm.input_w_: w,\n",
    "            lm.target_y_: y,\n",
    "            lm.initial_h_: h,\n",
    "            lm.learning_rate_: learning_rate,\n",
    "            lm.use_dropout_: use_dropout\n",
    "        }\n",
    "        ops = [loss, lm.final_h_, train_op]        \n",
    "        #### YOUR CODE HERE ####\n",
    "        # session.run(...) the ops with the feed_dict constructed above.\n",
    "        # Ensure \"cost\" becomes the value of \"loss\".\n",
    "        # Hint: see \"ops\" for other variables that need updating in this loop.\n",
    "        cost,h,_ = session.run(ops,feed_dict=feed_dict)\n",
    "\n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=0.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the cell below to verify your implementation of `run_epoch`, and to test your RNN on a (very simple) toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload(rnnlm); reload(rnnlm_test)\n",
    "th = rnnlm_test.RunEpochTester(\"test_toy_model\")\n",
    "th.setUp(); th.injectCode(run_epoch, score_dataset)\n",
    "unittest.TextTestRunner(verbosity=2).run(th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as above, this is a *very* simple test case that does not guarantee model correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run Training\n",
    "\n",
    "We'll give you the outline of the training procedure, but you'll need to fill in a call to your `run_epoch` function. \n",
    "\n",
    "At the end of training, we use a `tf.train.Saver` to save a copy of the model to `/tmp/w266/a3_model/rnnlm_trained`. You'll want to load this from disk to work on later parts of the assignment; see **part (d)** for an example of how this is done.\n",
    "\n",
    "#### Tuning Hyperparameters\n",
    "With a sampled softmax loss, the default hyperparameters should train 5 epochs in around 15 minutes on a single-core GCE instance, and reach a training set perplexity between 120-140.\n",
    "\n",
    "However, it's possible to do significantly better. Try experimenting with multiple RNN layers (`num_layers` > 1) or a larger hidden state - though you may also need to adjust the learning rate and number of epochs for a larger model.\n",
    "\n",
    "You can also experiment with a larger vocabulary. This will look worse for perplexity, but will be a better model overall as it won't treat so many words as `<unk>`.\n",
    "\n",
    "#### Notes on Speed\n",
    "\n",
    "To speed things up, you may want to re-start your GCE instance with more CPUs. Using a 16-core machine will train *very* quickly if using a sampled softmax lost, almost as fast as a GPU. (Because of the sequential nature of the model, GPUs aren't actually much faster than CPUs for training and running RNNs.) The training code will print the words-per-second processed; with the default settings on a single core, you can expect around 8000 WPS, or up to more than 25000 WPS on a fast multi-core machine.\n",
    "\n",
    "You might also want to modify the code below to only run score_dataset at the very end, after all epochs are completed. This will speed things up significantly, since `score_dataset` uses the full softmax loss - and so often can take longer than a whole training epoch!\n",
    "\n",
    "#### Submitting your model\n",
    "You should submit your trained model along with the assignment. Do:\n",
    "```\n",
    "cp /tmp/w266/a3_model/rnnlm_trained* .\n",
    "git add rnnlm_trained*\n",
    "git commit -m \"Adding trained model.\"\n",
    "```\n",
    "Unless you train a very large model, these files should be < 50 MB and no problem for git to handle. If you do also train a large model, please only submit the smaller one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/nconidas/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "Vocabulary: 10,000 types\n",
      "Loaded 57,340 sentences (1.16119e+06 tokens)\n",
      "Training set: 45,872 sentences (924,077 tokens)\n",
      "Test set: 11,468 sentences (237,115 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "V = 10000\n",
    "vocab, train_ids, test_ids = utils.load_corpus(\"brown\", split=0.8, V=V, shuffle=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d0fc0ec6bff9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Model parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m model_params = dict(V=vocab.size, \n\u001b[0m\u001b[0;32m      9\u001b[0m                     \u001b[0mH\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                     \u001b[0msoftmax_ns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "max_time = 25\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, \n",
    "                    H=200, \n",
    "                    softmax_ns=200,\n",
    "                    num_layers=2)\n",
    "\n",
    "TF_SAVEDIR = \"/tmp/w266/a3_model\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[batch 0]: seen 2500 words at 2437.0 wps, loss = 7.950\n",
      "[batch 2]: seen 7500 words at 2836.4 wps, loss = 7.549\n",
      "[batch 4]: seen 12500 words at 2936.2 wps, loss = 7.463\n",
      "[batch 6]: seen 17500 words at 2978.0 wps, loss = 7.324\n",
      "[batch 8]: seen 22500 words at 2997.1 wps, loss = 7.137\n",
      "[batch 10]: seen 27500 words at 3032.5 wps, loss = 6.981\n",
      "[batch 12]: seen 32500 words at 3067.7 wps, loss = 6.874\n",
      "[batch 14]: seen 37500 words at 3071.3 wps, loss = 6.764\n",
      "[batch 16]: seen 42500 words at 3088.3 wps, loss = 6.652\n",
      "[batch 18]: seen 47500 words at 3096.7 wps, loss = 6.561\n",
      "[batch 20]: seen 52500 words at 3111.6 wps, loss = 6.490\n",
      "[batch 22]: seen 57500 words at 3120.9 wps, loss = 6.408\n",
      "[batch 24]: seen 62500 words at 3133.7 wps, loss = 6.337\n",
      "[batch 26]: seen 67500 words at 3141.4 wps, loss = 6.275\n",
      "[batch 28]: seen 72500 words at 3150.6 wps, loss = 6.205\n",
      "[batch 30]: seen 77500 words at 3159.0 wps, loss = 6.139\n",
      "[batch 32]: seen 82500 words at 3164.5 wps, loss = 6.076\n",
      "[batch 34]: seen 87500 words at 3157.2 wps, loss = 6.027\n",
      "[batch 36]: seen 92500 words at 3147.7 wps, loss = 5.977\n",
      "[batch 38]: seen 97500 words at 3141.5 wps, loss = 5.934\n",
      "[batch 40]: seen 102500 words at 3131.4 wps, loss = 5.893\n",
      "[batch 42]: seen 107500 words at 3119.1 wps, loss = 5.851\n",
      "[batch 44]: seen 112500 words at 3115.4 wps, loss = 5.813\n",
      "[batch 46]: seen 117500 words at 3112.1 wps, loss = 5.774\n",
      "[batch 48]: seen 122500 words at 3113.1 wps, loss = 5.738\n",
      "[batch 50]: seen 127500 words at 3115.3 wps, loss = 5.702\n",
      "[batch 52]: seen 132500 words at 3116.8 wps, loss = 5.671\n",
      "[batch 54]: seen 137500 words at 3114.4 wps, loss = 5.642\n",
      "[batch 56]: seen 142500 words at 3119.6 wps, loss = 5.615\n",
      "[batch 58]: seen 147500 words at 3116.0 wps, loss = 5.590\n",
      "[batch 60]: seen 152500 words at 3113.9 wps, loss = 5.566\n",
      "[batch 62]: seen 157500 words at 3114.3 wps, loss = 5.541\n",
      "[batch 64]: seen 162500 words at 3120.7 wps, loss = 5.519\n",
      "[batch 66]: seen 167500 words at 3125.8 wps, loss = 5.497\n",
      "[batch 68]: seen 172500 words at 3130.7 wps, loss = 5.476\n",
      "[batch 70]: seen 177500 words at 3130.5 wps, loss = 5.454\n",
      "[batch 72]: seen 182500 words at 3130.5 wps, loss = 5.436\n",
      "[batch 74]: seen 187500 words at 3133.2 wps, loss = 5.418\n",
      "[batch 76]: seen 192500 words at 3137.2 wps, loss = 5.401\n",
      "[batch 78]: seen 197500 words at 3139.7 wps, loss = 5.384\n",
      "[batch 80]: seen 202500 words at 3138.7 wps, loss = 5.367\n",
      "[batch 82]: seen 207500 words at 3138.9 wps, loss = 5.353\n",
      "[batch 84]: seen 212500 words at 3139.4 wps, loss = 5.338\n",
      "[batch 86]: seen 217500 words at 3138.9 wps, loss = 5.324\n",
      "[batch 88]: seen 222500 words at 3141.9 wps, loss = 5.309\n",
      "[batch 90]: seen 227500 words at 3143.1 wps, loss = 5.297\n",
      "[batch 92]: seen 232500 words at 3145.2 wps, loss = 5.286\n",
      "[batch 94]: seen 237500 words at 3144.3 wps, loss = 5.272\n",
      "[batch 96]: seen 242500 words at 3143.5 wps, loss = 5.260\n",
      "[batch 98]: seen 247500 words at 3144.1 wps, loss = 5.248\n",
      "[batch 100]: seen 252500 words at 3144.1 wps, loss = 5.237\n",
      "[batch 102]: seen 257500 words at 3143.4 wps, loss = 5.226\n",
      "[batch 104]: seen 262500 words at 3143.2 wps, loss = 5.214\n",
      "[batch 106]: seen 267500 words at 3144.0 wps, loss = 5.202\n",
      "[batch 108]: seen 272500 words at 3143.7 wps, loss = 5.193\n",
      "[batch 110]: seen 277500 words at 3144.1 wps, loss = 5.183\n",
      "[batch 112]: seen 282500 words at 3143.4 wps, loss = 5.174\n",
      "[batch 114]: seen 287500 words at 3142.2 wps, loss = 5.165\n",
      "[batch 116]: seen 292500 words at 3142.0 wps, loss = 5.156\n",
      "[batch 118]: seen 297500 words at 3141.4 wps, loss = 5.147\n",
      "[batch 120]: seen 302500 words at 3140.5 wps, loss = 5.138\n",
      "[batch 122]: seen 307500 words at 3139.4 wps, loss = 5.130\n",
      "[batch 124]: seen 312500 words at 3138.8 wps, loss = 5.123\n",
      "[batch 126]: seen 317500 words at 3138.9 wps, loss = 5.114\n",
      "[batch 128]: seen 322500 words at 3141.1 wps, loss = 5.106\n",
      "[batch 130]: seen 327500 words at 3140.6 wps, loss = 5.100\n",
      "[batch 132]: seen 332500 words at 3139.5 wps, loss = 5.091\n",
      "[batch 134]: seen 337500 words at 3139.5 wps, loss = 5.082\n",
      "[batch 136]: seen 342500 words at 3138.9 wps, loss = 5.074\n",
      "[batch 138]: seen 347500 words at 3138.4 wps, loss = 5.066\n",
      "[batch 140]: seen 352500 words at 3138.5 wps, loss = 5.058\n",
      "[batch 142]: seen 357500 words at 3137.4 wps, loss = 5.051\n",
      "[batch 144]: seen 362500 words at 3137.6 wps, loss = 5.045\n",
      "[batch 146]: seen 367500 words at 3137.0 wps, loss = 5.038\n",
      "[batch 148]: seen 372500 words at 3135.9 wps, loss = 5.031\n",
      "[batch 150]: seen 377500 words at 3137.6 wps, loss = 5.025\n",
      "[batch 152]: seen 382500 words at 3138.9 wps, loss = 5.017\n",
      "[batch 154]: seen 387500 words at 3140.5 wps, loss = 5.011\n",
      "[batch 156]: seen 392500 words at 3142.1 wps, loss = 5.004\n",
      "[batch 158]: seen 397500 words at 3143.7 wps, loss = 4.999\n",
      "[batch 160]: seen 402500 words at 3144.4 wps, loss = 4.993\n",
      "[batch 162]: seen 407500 words at 3145.0 wps, loss = 4.988\n",
      "[batch 164]: seen 412500 words at 3145.5 wps, loss = 4.981\n",
      "[batch 166]: seen 417500 words at 3147.2 wps, loss = 4.976\n",
      "[batch 168]: seen 422500 words at 3149.0 wps, loss = 4.970\n",
      "[batch 170]: seen 427500 words at 3150.0 wps, loss = 4.965\n",
      "[batch 172]: seen 432500 words at 3149.0 wps, loss = 4.960\n",
      "[batch 174]: seen 437500 words at 3148.3 wps, loss = 4.954\n",
      "[batch 176]: seen 442500 words at 3149.2 wps, loss = 4.948\n",
      "[batch 178]: seen 447500 words at 3150.7 wps, loss = 4.943\n",
      "[batch 180]: seen 452500 words at 3152.3 wps, loss = 4.938\n",
      "[batch 182]: seen 457500 words at 3153.9 wps, loss = 4.932\n",
      "[batch 184]: seen 462500 words at 3155.3 wps, loss = 4.927\n",
      "[batch 186]: seen 467500 words at 3156.6 wps, loss = 4.923\n",
      "[batch 188]: seen 472500 words at 3157.6 wps, loss = 4.917\n",
      "[batch 190]: seen 477500 words at 3158.5 wps, loss = 4.912\n",
      "[batch 192]: seen 482500 words at 3159.4 wps, loss = 4.905\n",
      "[batch 194]: seen 487500 words at 3159.5 wps, loss = 4.901\n",
      "[batch 196]: seen 492500 words at 3159.6 wps, loss = 4.897\n",
      "[batch 198]: seen 497500 words at 3159.6 wps, loss = 4.892\n",
      "[batch 200]: seen 502500 words at 3158.6 wps, loss = 4.887\n",
      "[batch 202]: seen 507500 words at 3158.0 wps, loss = 4.882\n",
      "[batch 204]: seen 512500 words at 3159.0 wps, loss = 4.879\n",
      "[batch 206]: seen 517500 words at 3158.9 wps, loss = 4.875\n",
      "[batch 208]: seen 522500 words at 3159.9 wps, loss = 4.871\n",
      "[batch 210]: seen 527500 words at 3160.8 wps, loss = 4.866\n",
      "[batch 212]: seen 532500 words at 3160.2 wps, loss = 4.861\n",
      "[batch 214]: seen 537500 words at 3159.9 wps, loss = 4.857\n",
      "[batch 216]: seen 542500 words at 3160.1 wps, loss = 4.852\n",
      "[batch 218]: seen 547500 words at 3160.0 wps, loss = 4.850\n",
      "[batch 220]: seen 552500 words at 3159.4 wps, loss = 4.846\n",
      "[batch 222]: seen 557500 words at 3158.4 wps, loss = 4.842\n",
      "[batch 224]: seen 562500 words at 3157.8 wps, loss = 4.838\n",
      "[batch 226]: seen 567500 words at 3157.3 wps, loss = 4.834\n",
      "[batch 228]: seen 572500 words at 3156.4 wps, loss = 4.830\n",
      "[batch 230]: seen 577500 words at 3157.1 wps, loss = 4.827\n",
      "[batch 232]: seen 582500 words at 3158.1 wps, loss = 4.823\n",
      "[batch 234]: seen 587500 words at 3159.4 wps, loss = 4.819\n",
      "[batch 236]: seen 592500 words at 3160.4 wps, loss = 4.815\n",
      "[batch 238]: seen 597500 words at 3161.6 wps, loss = 4.812\n",
      "[batch 240]: seen 602500 words at 3162.4 wps, loss = 4.808\n",
      "[batch 242]: seen 607500 words at 3163.5 wps, loss = 4.804\n",
      "[batch 244]: seen 612500 words at 3163.9 wps, loss = 4.800\n",
      "[batch 246]: seen 617500 words at 3164.8 wps, loss = 4.797\n",
      "[batch 248]: seen 622500 words at 3163.6 wps, loss = 4.793\n",
      "[batch 250]: seen 627500 words at 3162.7 wps, loss = 4.789\n",
      "[batch 252]: seen 632500 words at 3162.2 wps, loss = 4.785\n",
      "[batch 254]: seen 637500 words at 3161.5 wps, loss = 4.782\n",
      "[batch 256]: seen 642500 words at 3160.6 wps, loss = 4.779\n",
      "[batch 258]: seen 647500 words at 3160.4 wps, loss = 4.776\n",
      "[batch 260]: seen 652500 words at 3159.6 wps, loss = 4.772\n",
      "[batch 262]: seen 657500 words at 3160.5 wps, loss = 4.769\n",
      "[batch 264]: seen 662500 words at 3160.2 wps, loss = 4.766\n",
      "[batch 266]: seen 667500 words at 3159.5 wps, loss = 4.763\n",
      "[batch 268]: seen 672500 words at 3159.1 wps, loss = 4.760\n",
      "[batch 270]: seen 677500 words at 3158.6 wps, loss = 4.756\n",
      "[batch 272]: seen 682500 words at 3158.0 wps, loss = 4.753\n",
      "[batch 274]: seen 687500 words at 3158.6 wps, loss = 4.750\n",
      "[batch 276]: seen 692500 words at 3159.3 wps, loss = 4.747\n",
      "[batch 278]: seen 697500 words at 3158.2 wps, loss = 4.745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 280]: seen 702500 words at 3157.3 wps, loss = 4.742\n",
      "[batch 282]: seen 707500 words at 3156.6 wps, loss = 4.739\n",
      "[batch 284]: seen 712500 words at 3156.2 wps, loss = 4.735\n",
      "[batch 286]: seen 717500 words at 3157.0 wps, loss = 4.733\n",
      "[batch 288]: seen 722500 words at 3157.0 wps, loss = 4.730\n",
      "[batch 290]: seen 727500 words at 3156.1 wps, loss = 4.727\n",
      "[batch 292]: seen 732500 words at 3155.3 wps, loss = 4.725\n",
      "[batch 294]: seen 737500 words at 3154.9 wps, loss = 4.722\n",
      "[batch 296]: seen 742500 words at 3154.3 wps, loss = 4.719\n",
      "[batch 298]: seen 747500 words at 3154.2 wps, loss = 4.716\n",
      "[batch 300]: seen 752500 words at 3154.7 wps, loss = 4.713\n",
      "[batch 302]: seen 757500 words at 3154.6 wps, loss = 4.710\n",
      "[batch 304]: seen 762500 words at 3154.3 wps, loss = 4.707\n",
      "[batch 306]: seen 767500 words at 3153.7 wps, loss = 4.704\n",
      "[batch 308]: seen 772500 words at 3153.6 wps, loss = 4.702\n",
      "[batch 310]: seen 777500 words at 3154.4 wps, loss = 4.699\n",
      "[batch 312]: seen 782500 words at 3155.2 wps, loss = 4.696\n",
      "[batch 314]: seen 787500 words at 3154.8 wps, loss = 4.694\n",
      "[batch 316]: seen 792500 words at 3154.8 wps, loss = 4.691\n",
      "[batch 318]: seen 797500 words at 3155.5 wps, loss = 4.689\n",
      "[batch 320]: seen 802500 words at 3155.5 wps, loss = 4.686\n",
      "[batch 322]: seen 807500 words at 3155.0 wps, loss = 4.683\n",
      "[batch 324]: seen 812500 words at 3155.1 wps, loss = 4.681\n",
      "[batch 326]: seen 817500 words at 3155.6 wps, loss = 4.679\n",
      "[batch 328]: seen 822500 words at 3155.0 wps, loss = 4.676\n",
      "[batch 330]: seen 827500 words at 3154.3 wps, loss = 4.673\n",
      "[batch 332]: seen 832500 words at 3154.7 wps, loss = 4.671\n",
      "[batch 334]: seen 837500 words at 3155.0 wps, loss = 4.668\n",
      "[batch 336]: seen 842500 words at 3155.5 wps, loss = 4.666\n",
      "[batch 338]: seen 847500 words at 3156.0 wps, loss = 4.664\n",
      "[batch 340]: seen 852500 words at 3156.7 wps, loss = 4.661\n",
      "[batch 342]: seen 857500 words at 3156.4 wps, loss = 4.659\n",
      "[batch 344]: seen 862500 words at 3156.0 wps, loss = 4.657\n",
      "[batch 346]: seen 867500 words at 3155.8 wps, loss = 4.655\n",
      "[batch 348]: seen 872500 words at 3155.7 wps, loss = 4.653\n",
      "[batch 350]: seen 877500 words at 3155.4 wps, loss = 4.650\n",
      "[batch 352]: seen 882500 words at 3155.2 wps, loss = 4.648\n",
      "[batch 354]: seen 887500 words at 3155.1 wps, loss = 4.646\n",
      "[batch 356]: seen 892500 words at 3154.7 wps, loss = 4.644\n",
      "[batch 358]: seen 897500 words at 3154.0 wps, loss = 4.641\n",
      "[batch 360]: seen 902500 words at 3153.7 wps, loss = 4.638\n",
      "[batch 362]: seen 907500 words at 3153.5 wps, loss = 4.636\n",
      "[batch 364]: seen 912500 words at 3153.3 wps, loss = 4.634\n",
      "[batch 366]: seen 917500 words at 3153.1 wps, loss = 4.632\n",
      "[batch 368]: seen 922500 words at 3152.7 wps, loss = 4.630\n",
      "[batch 370]: seen 927500 words at 3152.5 wps, loss = 4.629\n",
      "[batch 372]: seen 932500 words at 3152.1 wps, loss = 4.627\n",
      "[batch 374]: seen 937500 words at 3152.4 wps, loss = 4.625\n",
      "[batch 376]: seen 942500 words at 3153.2 wps, loss = 4.623\n",
      "[batch 378]: seen 947500 words at 3154.0 wps, loss = 4.620\n",
      "[batch 380]: seen 952500 words at 3154.7 wps, loss = 4.618\n",
      "[batch 382]: seen 957500 words at 3154.6 wps, loss = 4.616\n",
      "[batch 384]: seen 962500 words at 3154.9 wps, loss = 4.614\n",
      "[batch 386]: seen 967500 words at 3155.6 wps, loss = 4.612\n",
      "[epoch 1] Completed in 0:05:07\n",
      "[epoch 2] Starting epoch 2\n",
      "[batch 1]: seen 5000 words at 3163.5 wps, loss = 4.206\n",
      "[batch 3]: seen 10000 words at 3200.8 wps, loss = 4.191\n",
      "[batch 5]: seen 15000 words at 3217.8 wps, loss = 4.180\n",
      "[batch 7]: seen 20000 words at 3208.3 wps, loss = 4.181\n",
      "[batch 9]: seen 25000 words at 3180.5 wps, loss = 4.198\n",
      "[batch 11]: seen 30000 words at 3173.7 wps, loss = 4.201\n",
      "[batch 13]: seen 35000 words at 3188.2 wps, loss = 4.204\n",
      "[batch 15]: seen 40000 words at 3195.4 wps, loss = 4.200\n",
      "[batch 17]: seen 45000 words at 3203.6 wps, loss = 4.199\n",
      "[batch 19]: seen 50000 words at 3195.1 wps, loss = 4.200\n",
      "[batch 21]: seen 55000 words at 3179.9 wps, loss = 4.202\n",
      "[batch 23]: seen 60000 words at 3170.5 wps, loss = 4.198\n",
      "[batch 25]: seen 65000 words at 3159.9 wps, loss = 4.196\n",
      "[batch 27]: seen 70000 words at 3154.6 wps, loss = 4.192\n",
      "[batch 29]: seen 75000 words at 3148.4 wps, loss = 4.188\n",
      "[batch 31]: seen 80000 words at 3148.4 wps, loss = 4.183\n",
      "[batch 33]: seen 85000 words at 3143.4 wps, loss = 4.183\n",
      "[batch 35]: seen 90000 words at 3138.5 wps, loss = 4.184\n",
      "[batch 37]: seen 95000 words at 3135.7 wps, loss = 4.186\n",
      "[batch 39]: seen 100000 words at 3132.1 wps, loss = 4.187\n",
      "[batch 41]: seen 105000 words at 3118.6 wps, loss = 4.188\n",
      "[batch 43]: seen 110000 words at 3113.0 wps, loss = 4.188\n",
      "[batch 45]: seen 115000 words at 3104.6 wps, loss = 4.188\n",
      "[batch 47]: seen 120000 words at 3096.8 wps, loss = 4.190\n",
      "[batch 49]: seen 125000 words at 3091.4 wps, loss = 4.190\n",
      "[batch 51]: seen 130000 words at 3088.6 wps, loss = 4.189\n",
      "[batch 53]: seen 135000 words at 3090.1 wps, loss = 4.186\n",
      "[batch 55]: seen 140000 words at 3093.2 wps, loss = 4.187\n",
      "[batch 57]: seen 145000 words at 3095.9 wps, loss = 4.185\n",
      "[batch 59]: seen 150000 words at 3100.9 wps, loss = 4.182\n",
      "[batch 61]: seen 155000 words at 3106.9 wps, loss = 4.181\n",
      "[batch 63]: seen 160000 words at 3111.9 wps, loss = 4.179\n",
      "[batch 65]: seen 165000 words at 3115.9 wps, loss = 4.176\n",
      "[batch 67]: seen 170000 words at 3120.2 wps, loss = 4.175\n",
      "[batch 69]: seen 175000 words at 3118.8 wps, loss = 4.173\n",
      "[batch 71]: seen 180000 words at 3117.7 wps, loss = 4.171\n",
      "[batch 73]: seen 185000 words at 3116.5 wps, loss = 4.171\n",
      "[batch 75]: seen 190000 words at 3116.0 wps, loss = 4.170\n",
      "[batch 77]: seen 195000 words at 3116.2 wps, loss = 4.172\n",
      "[batch 79]: seen 200000 words at 3115.1 wps, loss = 4.171\n",
      "[batch 81]: seen 205000 words at 3114.4 wps, loss = 4.173\n",
      "[batch 83]: seen 210000 words at 3114.5 wps, loss = 4.172\n",
      "[batch 85]: seen 215000 words at 3114.2 wps, loss = 4.171\n",
      "[batch 87]: seen 220000 words at 3116.2 wps, loss = 4.170\n",
      "[batch 89]: seen 225000 words at 3120.0 wps, loss = 4.169\n",
      "[batch 91]: seen 230000 words at 3122.5 wps, loss = 4.169\n",
      "[batch 93]: seen 235000 words at 3122.0 wps, loss = 4.169\n",
      "[batch 95]: seen 240000 words at 3121.7 wps, loss = 4.167\n",
      "[batch 97]: seen 245000 words at 3121.6 wps, loss = 4.166\n",
      "[batch 99]: seen 250000 words at 3121.7 wps, loss = 4.167\n",
      "[batch 101]: seen 255000 words at 3121.4 wps, loss = 4.167\n",
      "[batch 103]: seen 260000 words at 3121.2 wps, loss = 4.167\n",
      "[batch 105]: seen 265000 words at 3120.4 wps, loss = 4.166\n",
      "[batch 107]: seen 270000 words at 3120.2 wps, loss = 4.164\n",
      "[batch 109]: seen 275000 words at 3121.1 wps, loss = 4.163\n",
      "[batch 111]: seen 280000 words at 3123.9 wps, loss = 4.163\n",
      "[batch 113]: seen 285000 words at 3126.2 wps, loss = 4.161\n",
      "[batch 115]: seen 290000 words at 3127.9 wps, loss = 4.161\n",
      "[batch 117]: seen 295000 words at 3129.8 wps, loss = 4.159\n",
      "[batch 119]: seen 300000 words at 3129.1 wps, loss = 4.159\n",
      "[batch 121]: seen 305000 words at 3128.6 wps, loss = 4.159\n",
      "[batch 123]: seen 310000 words at 3128.2 wps, loss = 4.158\n",
      "[batch 125]: seen 315000 words at 3130.7 wps, loss = 4.158\n",
      "[batch 127]: seen 320000 words at 3133.0 wps, loss = 4.156\n",
      "[batch 129]: seen 325000 words at 3135.3 wps, loss = 4.156\n",
      "[batch 131]: seen 330000 words at 3137.1 wps, loss = 4.155\n",
      "[batch 133]: seen 335000 words at 3137.1 wps, loss = 4.153\n",
      "[batch 135]: seen 340000 words at 3138.6 wps, loss = 4.152\n",
      "[batch 137]: seen 345000 words at 3139.4 wps, loss = 4.150\n",
      "[batch 139]: seen 350000 words at 3140.3 wps, loss = 4.149\n",
      "[batch 141]: seen 355000 words at 3141.1 wps, loss = 4.148\n",
      "[batch 143]: seen 360000 words at 3139.8 wps, loss = 4.147\n",
      "[batch 145]: seen 365000 words at 3139.8 wps, loss = 4.146\n",
      "[batch 147]: seen 370000 words at 3141.8 wps, loss = 4.145\n",
      "[batch 149]: seen 375000 words at 3143.7 wps, loss = 4.144\n",
      "[batch 151]: seen 380000 words at 3145.4 wps, loss = 4.145\n",
      "[batch 153]: seen 385000 words at 3145.4 wps, loss = 4.144\n",
      "[batch 155]: seen 390000 words at 3144.6 wps, loss = 4.144\n",
      "[batch 157]: seen 395000 words at 3145.2 wps, loss = 4.142\n",
      "[batch 159]: seen 400000 words at 3144.7 wps, loss = 4.142\n",
      "[batch 161]: seen 405000 words at 3146.3 wps, loss = 4.141\n",
      "[batch 163]: seen 410000 words at 3147.7 wps, loss = 4.142\n",
      "[batch 165]: seen 415000 words at 3148.7 wps, loss = 4.142\n",
      "[batch 167]: seen 420000 words at 3150.4 wps, loss = 4.142\n",
      "[batch 169]: seen 425000 words at 3152.0 wps, loss = 4.141\n",
      "[batch 171]: seen 430000 words at 3153.1 wps, loss = 4.141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 173]: seen 435000 words at 3152.2 wps, loss = 4.140\n",
      "[batch 175]: seen 440000 words at 3151.8 wps, loss = 4.139\n",
      "[batch 177]: seen 445000 words at 3150.5 wps, loss = 4.138\n",
      "[batch 179]: seen 450000 words at 3150.1 wps, loss = 4.139\n",
      "[batch 181]: seen 455000 words at 3151.6 wps, loss = 4.138\n",
      "[batch 183]: seen 460000 words at 3153.2 wps, loss = 4.136\n",
      "[batch 185]: seen 465000 words at 3154.0 wps, loss = 4.135\n",
      "[batch 187]: seen 470000 words at 3155.2 wps, loss = 4.135\n",
      "[batch 189]: seen 475000 words at 3156.5 wps, loss = 4.135\n",
      "[batch 191]: seen 480000 words at 3157.5 wps, loss = 4.133\n",
      "[batch 193]: seen 485000 words at 3159.0 wps, loss = 4.133\n",
      "[batch 195]: seen 490000 words at 3160.4 wps, loss = 4.132\n",
      "[batch 197]: seen 495000 words at 3161.8 wps, loss = 4.131\n",
      "[batch 199]: seen 500000 words at 3163.1 wps, loss = 4.130\n",
      "[batch 201]: seen 505000 words at 3164.7 wps, loss = 4.130\n",
      "[batch 203]: seen 510000 words at 3165.8 wps, loss = 4.129\n",
      "[batch 205]: seen 515000 words at 3167.0 wps, loss = 4.129\n",
      "[batch 207]: seen 520000 words at 3168.2 wps, loss = 4.128\n",
      "[batch 209]: seen 525000 words at 3169.5 wps, loss = 4.128\n",
      "[batch 211]: seen 530000 words at 3170.6 wps, loss = 4.128\n",
      "[batch 213]: seen 535000 words at 3171.5 wps, loss = 4.127\n",
      "[batch 215]: seen 540000 words at 3172.6 wps, loss = 4.126\n",
      "[batch 217]: seen 545000 words at 3172.9 wps, loss = 4.125\n",
      "[batch 219]: seen 550000 words at 3172.1 wps, loss = 4.125\n",
      "[batch 221]: seen 555000 words at 3171.5 wps, loss = 4.124\n",
      "[batch 223]: seen 560000 words at 3170.9 wps, loss = 4.123\n",
      "[batch 225]: seen 565000 words at 3170.7 wps, loss = 4.123\n",
      "[batch 227]: seen 570000 words at 3169.9 wps, loss = 4.122\n",
      "[batch 229]: seen 575000 words at 3169.5 wps, loss = 4.122\n",
      "[batch 231]: seen 580000 words at 3170.4 wps, loss = 4.121\n",
      "[batch 233]: seen 585000 words at 3170.1 wps, loss = 4.121\n",
      "[batch 235]: seen 590000 words at 3170.9 wps, loss = 4.120\n",
      "[batch 237]: seen 595000 words at 3171.5 wps, loss = 4.120\n",
      "[batch 239]: seen 600000 words at 3171.7 wps, loss = 4.119\n",
      "[batch 241]: seen 605000 words at 3170.9 wps, loss = 4.118\n",
      "[batch 243]: seen 610000 words at 3170.9 wps, loss = 4.118\n",
      "[batch 245]: seen 615000 words at 3170.0 wps, loss = 4.117\n",
      "[batch 247]: seen 620000 words at 3169.2 wps, loss = 4.116\n",
      "[batch 249]: seen 625000 words at 3168.5 wps, loss = 4.115\n",
      "[batch 251]: seen 630000 words at 3167.7 wps, loss = 4.114\n",
      "[batch 253]: seen 635000 words at 3167.0 wps, loss = 4.113\n",
      "[batch 255]: seen 640000 words at 3166.5 wps, loss = 4.113\n",
      "[batch 257]: seen 645000 words at 3166.0 wps, loss = 4.113\n",
      "[batch 259]: seen 650000 words at 3165.4 wps, loss = 4.112\n",
      "[batch 261]: seen 655000 words at 3165.0 wps, loss = 4.111\n",
      "[batch 263]: seen 660000 words at 3165.3 wps, loss = 4.111\n",
      "[batch 265]: seen 665000 words at 3165.3 wps, loss = 4.111\n",
      "[batch 267]: seen 670000 words at 3164.9 wps, loss = 4.111\n",
      "[batch 269]: seen 675000 words at 3165.0 wps, loss = 4.110\n",
      "[batch 271]: seen 680000 words at 3165.0 wps, loss = 4.110\n",
      "[batch 273]: seen 685000 words at 3163.5 wps, loss = 4.110\n",
      "[batch 275]: seen 690000 words at 3163.8 wps, loss = 4.109\n",
      "[batch 277]: seen 695000 words at 3163.8 wps, loss = 4.109\n",
      "[batch 279]: seen 700000 words at 3163.4 wps, loss = 4.108\n",
      "[batch 281]: seen 705000 words at 3162.8 wps, loss = 4.107\n",
      "[batch 283]: seen 710000 words at 3162.3 wps, loss = 4.107\n",
      "[batch 285]: seen 715000 words at 3161.9 wps, loss = 4.107\n",
      "[batch 287]: seen 720000 words at 3162.2 wps, loss = 4.106\n",
      "[batch 289]: seen 725000 words at 3163.1 wps, loss = 4.106\n",
      "[batch 291]: seen 730000 words at 3164.1 wps, loss = 4.105\n",
      "[batch 293]: seen 735000 words at 3165.0 wps, loss = 4.105\n",
      "[batch 295]: seen 740000 words at 3164.5 wps, loss = 4.104\n",
      "[batch 297]: seen 745000 words at 3164.5 wps, loss = 4.104\n",
      "[batch 299]: seen 750000 words at 3164.3 wps, loss = 4.103\n",
      "[batch 301]: seen 755000 words at 3163.9 wps, loss = 4.103\n",
      "[batch 303]: seen 760000 words at 3163.5 wps, loss = 4.102\n",
      "[batch 305]: seen 765000 words at 3163.9 wps, loss = 4.102\n",
      "[batch 307]: seen 770000 words at 3164.8 wps, loss = 4.101\n",
      "[batch 309]: seen 775000 words at 3165.1 wps, loss = 4.100\n",
      "[batch 311]: seen 780000 words at 3164.6 wps, loss = 4.100\n",
      "[batch 313]: seen 785000 words at 3164.2 wps, loss = 4.099\n",
      "[batch 315]: seen 790000 words at 3164.6 wps, loss = 4.099\n",
      "[batch 317]: seen 795000 words at 3165.1 wps, loss = 4.098\n",
      "[batch 319]: seen 800000 words at 3164.5 wps, loss = 4.098\n",
      "[batch 321]: seen 805000 words at 3163.9 wps, loss = 4.097\n",
      "[batch 323]: seen 810000 words at 3163.1 wps, loss = 4.096\n",
      "[batch 325]: seen 815000 words at 3163.0 wps, loss = 4.095\n",
      "[batch 327]: seen 820000 words at 3162.9 wps, loss = 4.095\n",
      "[batch 329]: seen 825000 words at 3162.8 wps, loss = 4.095\n",
      "[batch 331]: seen 830000 words at 3162.4 wps, loss = 4.094\n",
      "[batch 333]: seen 835000 words at 3162.0 wps, loss = 4.093\n",
      "[batch 335]: seen 840000 words at 3162.4 wps, loss = 4.093\n",
      "[batch 337]: seen 845000 words at 3163.3 wps, loss = 4.093\n",
      "[batch 339]: seen 850000 words at 3164.3 wps, loss = 4.092\n",
      "[batch 341]: seen 855000 words at 3165.3 wps, loss = 4.091\n",
      "[batch 343]: seen 860000 words at 3166.1 wps, loss = 4.091\n",
      "[batch 345]: seen 865000 words at 3166.3 wps, loss = 4.091\n",
      "[batch 347]: seen 870000 words at 3165.8 wps, loss = 4.090\n",
      "[batch 349]: seen 875000 words at 3165.3 wps, loss = 4.090\n",
      "[batch 351]: seen 880000 words at 3164.5 wps, loss = 4.089\n",
      "[batch 353]: seen 885000 words at 3163.8 wps, loss = 4.088\n",
      "[batch 355]: seen 890000 words at 3164.5 wps, loss = 4.088\n",
      "[batch 357]: seen 895000 words at 3165.2 wps, loss = 4.087\n",
      "[batch 359]: seen 900000 words at 3165.8 wps, loss = 4.087\n",
      "[batch 361]: seen 905000 words at 3166.4 wps, loss = 4.086\n",
      "[batch 363]: seen 910000 words at 3165.9 wps, loss = 4.085\n",
      "[batch 365]: seen 915000 words at 3165.2 wps, loss = 4.084\n",
      "[batch 367]: seen 920000 words at 3164.8 wps, loss = 4.084\n",
      "[batch 369]: seen 925000 words at 3164.5 wps, loss = 4.084\n",
      "[batch 371]: seen 930000 words at 3164.1 wps, loss = 4.084\n",
      "[batch 373]: seen 935000 words at 3163.9 wps, loss = 4.083\n",
      "[batch 375]: seen 940000 words at 3163.6 wps, loss = 4.083\n",
      "[batch 377]: seen 945000 words at 3164.3 wps, loss = 4.082\n",
      "[batch 379]: seen 950000 words at 3164.8 wps, loss = 4.082\n",
      "[batch 381]: seen 955000 words at 3165.3 wps, loss = 4.082\n",
      "[batch 383]: seen 960000 words at 3165.9 wps, loss = 4.081\n",
      "[batch 385]: seen 965000 words at 3166.5 wps, loss = 4.081\n",
      "[batch 387]: seen 969900 words at 3166.7 wps, loss = 4.081\n",
      "[epoch 2] Completed in 0:05:06\n",
      "[epoch 3] Starting epoch 3\n",
      "[batch 1]: seen 5000 words at 3178.4 wps, loss = 4.027\n",
      "[batch 3]: seen 10000 words at 3186.8 wps, loss = 3.987\n",
      "[batch 5]: seen 15000 words at 3178.2 wps, loss = 3.988\n",
      "[batch 7]: seen 20000 words at 3166.6 wps, loss = 3.987\n",
      "[batch 9]: seen 25000 words at 3188.0 wps, loss = 3.981\n",
      "[batch 11]: seen 30000 words at 3207.4 wps, loss = 3.990\n",
      "[batch 13]: seen 35000 words at 3199.9 wps, loss = 3.989\n",
      "[batch 15]: seen 40000 words at 3202.6 wps, loss = 3.994\n",
      "[batch 17]: seen 45000 words at 3199.9 wps, loss = 3.994\n",
      "[batch 19]: seen 50000 words at 3189.7 wps, loss = 3.996\n",
      "[batch 21]: seen 55000 words at 3181.7 wps, loss = 3.997\n",
      "[batch 23]: seen 60000 words at 3189.2 wps, loss = 3.993\n",
      "[batch 25]: seen 65000 words at 3196.7 wps, loss = 3.987\n",
      "[batch 27]: seen 70000 words at 3202.8 wps, loss = 3.986\n",
      "[batch 29]: seen 75000 words at 3202.4 wps, loss = 3.985\n",
      "[batch 31]: seen 80000 words at 3210.7 wps, loss = 3.985\n",
      "[batch 33]: seen 85000 words at 3214.3 wps, loss = 3.986\n",
      "[batch 35]: seen 90000 words at 3219.5 wps, loss = 3.987\n",
      "[batch 37]: seen 95000 words at 3211.7 wps, loss = 3.988\n",
      "[batch 39]: seen 100000 words at 3216.7 wps, loss = 3.990\n",
      "[batch 41]: seen 105000 words at 3219.8 wps, loss = 3.987\n",
      "[batch 43]: seen 110000 words at 3221.6 wps, loss = 3.984\n",
      "[batch 45]: seen 115000 words at 3216.0 wps, loss = 3.984\n",
      "[batch 47]: seen 120000 words at 3201.2 wps, loss = 3.982\n",
      "[batch 49]: seen 125000 words at 3184.1 wps, loss = 3.980\n",
      "[batch 51]: seen 130000 words at 3174.5 wps, loss = 3.981\n",
      "[batch 53]: seen 135000 words at 3171.1 wps, loss = 3.977\n",
      "[batch 55]: seen 140000 words at 3166.5 wps, loss = 3.975\n",
      "[batch 57]: seen 145000 words at 3155.6 wps, loss = 3.976\n",
      "[batch 59]: seen 150000 words at 3151.0 wps, loss = 3.976\n",
      "[batch 61]: seen 155000 words at 3147.8 wps, loss = 3.977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 63]: seen 160000 words at 3148.7 wps, loss = 3.974\n",
      "[batch 65]: seen 165000 words at 3149.0 wps, loss = 3.976\n",
      "[batch 67]: seen 170000 words at 3152.3 wps, loss = 3.975\n",
      "[batch 69]: seen 175000 words at 3156.8 wps, loss = 3.975\n",
      "[batch 71]: seen 180000 words at 3161.0 wps, loss = 3.974\n",
      "[batch 73]: seen 185000 words at 3164.7 wps, loss = 3.973\n",
      "[batch 75]: seen 190000 words at 3168.3 wps, loss = 3.974\n",
      "[batch 77]: seen 195000 words at 3172.1 wps, loss = 3.973\n",
      "[batch 79]: seen 200000 words at 3175.7 wps, loss = 3.971\n",
      "[batch 81]: seen 205000 words at 3178.1 wps, loss = 3.972\n",
      "[batch 83]: seen 210000 words at 3179.9 wps, loss = 3.971\n",
      "[batch 85]: seen 215000 words at 3177.5 wps, loss = 3.969\n",
      "[batch 87]: seen 220000 words at 3177.7 wps, loss = 3.969\n",
      "[batch 89]: seen 225000 words at 3178.0 wps, loss = 3.967\n",
      "[batch 91]: seen 230000 words at 3176.3 wps, loss = 3.969\n",
      "[batch 93]: seen 235000 words at 3175.5 wps, loss = 3.968\n",
      "[batch 95]: seen 240000 words at 3176.4 wps, loss = 3.967\n",
      "[batch 97]: seen 245000 words at 3177.8 wps, loss = 3.967\n",
      "[batch 99]: seen 250000 words at 3179.3 wps, loss = 3.967\n",
      "[batch 101]: seen 255000 words at 3180.0 wps, loss = 3.968\n",
      "[batch 103]: seen 260000 words at 3181.0 wps, loss = 3.969\n",
      "[batch 105]: seen 265000 words at 3181.6 wps, loss = 3.967\n",
      "[batch 107]: seen 270000 words at 3180.1 wps, loss = 3.965\n",
      "[batch 109]: seen 275000 words at 3178.3 wps, loss = 3.964\n",
      "[batch 111]: seen 280000 words at 3178.0 wps, loss = 3.965\n",
      "[batch 113]: seen 285000 words at 3178.1 wps, loss = 3.963\n",
      "[batch 115]: seen 290000 words at 3178.8 wps, loss = 3.963\n",
      "[batch 117]: seen 295000 words at 3179.3 wps, loss = 3.963\n",
      "[batch 119]: seen 300000 words at 3178.9 wps, loss = 3.962\n",
      "[batch 121]: seen 305000 words at 3179.2 wps, loss = 3.961\n",
      "[batch 123]: seen 310000 words at 3179.9 wps, loss = 3.961\n",
      "[batch 125]: seen 315000 words at 3181.2 wps, loss = 3.961\n",
      "[batch 127]: seen 320000 words at 3181.6 wps, loss = 3.960\n",
      "[batch 129]: seen 325000 words at 3182.6 wps, loss = 3.960\n",
      "[batch 131]: seen 330000 words at 3182.9 wps, loss = 3.962\n",
      "[batch 133]: seen 335000 words at 3182.0 wps, loss = 3.961\n",
      "[batch 135]: seen 340000 words at 3180.6 wps, loss = 3.961\n",
      "[batch 137]: seen 345000 words at 3178.2 wps, loss = 3.960\n",
      "[batch 139]: seen 350000 words at 3178.7 wps, loss = 3.959\n",
      "[batch 141]: seen 355000 words at 3180.1 wps, loss = 3.959\n",
      "[batch 143]: seen 360000 words at 3181.5 wps, loss = 3.959\n",
      "[batch 145]: seen 365000 words at 3183.1 wps, loss = 3.959\n",
      "[batch 147]: seen 370000 words at 3183.5 wps, loss = 3.958\n",
      "[batch 149]: seen 375000 words at 3184.5 wps, loss = 3.957\n",
      "[batch 151]: seen 380000 words at 3183.2 wps, loss = 3.957\n",
      "[batch 153]: seen 385000 words at 3181.5 wps, loss = 3.956\n",
      "[batch 155]: seen 390000 words at 3180.5 wps, loss = 3.955\n",
      "[batch 157]: seen 395000 words at 3179.4 wps, loss = 3.954\n",
      "[batch 159]: seen 400000 words at 3180.4 wps, loss = 3.954\n",
      "[batch 161]: seen 405000 words at 3181.3 wps, loss = 3.954\n",
      "[batch 163]: seen 410000 words at 3181.8 wps, loss = 3.953\n",
      "[batch 165]: seen 415000 words at 3180.5 wps, loss = 3.952\n",
      "[batch 167]: seen 420000 words at 3179.1 wps, loss = 3.953\n",
      "[batch 169]: seen 425000 words at 3178.0 wps, loss = 3.952\n",
      "[batch 171]: seen 430000 words at 3176.8 wps, loss = 3.952\n",
      "[batch 173]: seen 435000 words at 3176.9 wps, loss = 3.951\n",
      "[batch 175]: seen 440000 words at 3177.9 wps, loss = 3.950\n",
      "[batch 177]: seen 445000 words at 3179.2 wps, loss = 3.950\n",
      "[batch 179]: seen 450000 words at 3180.2 wps, loss = 3.950\n",
      "[batch 181]: seen 455000 words at 3181.4 wps, loss = 3.950\n",
      "[batch 183]: seen 460000 words at 3182.3 wps, loss = 3.951\n",
      "[batch 185]: seen 465000 words at 3183.1 wps, loss = 3.951\n",
      "[batch 187]: seen 470000 words at 3184.0 wps, loss = 3.952\n",
      "[batch 189]: seen 475000 words at 3184.7 wps, loss = 3.952\n",
      "[batch 191]: seen 480000 words at 3184.7 wps, loss = 3.951\n",
      "[batch 193]: seen 485000 words at 3185.9 wps, loss = 3.951\n",
      "[batch 195]: seen 490000 words at 3186.6 wps, loss = 3.950\n",
      "[batch 197]: seen 495000 words at 3187.5 wps, loss = 3.951\n",
      "[batch 199]: seen 500000 words at 3187.8 wps, loss = 3.950\n",
      "[batch 201]: seen 505000 words at 3188.1 wps, loss = 3.950\n",
      "[batch 203]: seen 510000 words at 3187.8 wps, loss = 3.950\n",
      "[batch 205]: seen 515000 words at 3187.4 wps, loss = 3.950\n",
      "[batch 207]: seen 520000 words at 3188.7 wps, loss = 3.950\n",
      "[batch 209]: seen 525000 words at 3188.4 wps, loss = 3.949\n",
      "[batch 211]: seen 530000 words at 3187.3 wps, loss = 3.949\n",
      "[batch 213]: seen 535000 words at 3186.8 wps, loss = 3.949\n",
      "[batch 215]: seen 540000 words at 3186.1 wps, loss = 3.949\n",
      "[batch 217]: seen 545000 words at 3185.6 wps, loss = 3.948\n",
      "[batch 219]: seen 550000 words at 3184.5 wps, loss = 3.948\n",
      "[batch 221]: seen 555000 words at 3183.7 wps, loss = 3.948\n",
      "[batch 223]: seen 560000 words at 3182.9 wps, loss = 3.948\n",
      "[batch 225]: seen 565000 words at 3182.1 wps, loss = 3.948\n",
      "[batch 227]: seen 570000 words at 3181.5 wps, loss = 3.947\n",
      "[batch 229]: seen 575000 words at 3180.8 wps, loss = 3.947\n",
      "[batch 231]: seen 580000 words at 3180.3 wps, loss = 3.947\n",
      "[batch 233]: seen 585000 words at 3179.7 wps, loss = 3.947\n",
      "[batch 235]: seen 590000 words at 3179.7 wps, loss = 3.947\n",
      "[batch 237]: seen 595000 words at 3180.2 wps, loss = 3.947\n",
      "[batch 239]: seen 600000 words at 3181.0 wps, loss = 3.947\n",
      "[batch 241]: seen 605000 words at 3181.5 wps, loss = 3.946\n",
      "[batch 243]: seen 610000 words at 3182.2 wps, loss = 3.945\n",
      "[batch 245]: seen 615000 words at 3182.4 wps, loss = 3.945\n",
      "[batch 247]: seen 620000 words at 3182.9 wps, loss = 3.945\n",
      "[batch 249]: seen 625000 words at 3182.8 wps, loss = 3.944\n",
      "[batch 251]: seen 630000 words at 3182.2 wps, loss = 3.945\n",
      "[batch 253]: seen 635000 words at 3181.2 wps, loss = 3.944\n",
      "[batch 255]: seen 640000 words at 3180.3 wps, loss = 3.945\n",
      "[batch 257]: seen 645000 words at 3179.7 wps, loss = 3.944\n",
      "[batch 259]: seen 650000 words at 3180.3 wps, loss = 3.944\n",
      "[batch 261]: seen 655000 words at 3180.6 wps, loss = 3.944\n",
      "[batch 263]: seen 660000 words at 3179.9 wps, loss = 3.944\n",
      "[batch 265]: seen 665000 words at 3179.4 wps, loss = 3.943\n",
      "[batch 267]: seen 670000 words at 3179.6 wps, loss = 3.943\n",
      "[batch 269]: seen 675000 words at 3179.9 wps, loss = 3.943\n",
      "[batch 271]: seen 680000 words at 3180.3 wps, loss = 3.942\n",
      "[batch 273]: seen 685000 words at 3181.0 wps, loss = 3.942\n",
      "[batch 275]: seen 690000 words at 3181.4 wps, loss = 3.942\n",
      "[batch 277]: seen 695000 words at 3181.5 wps, loss = 3.941\n",
      "[batch 279]: seen 700000 words at 3181.3 wps, loss = 3.941\n",
      "[batch 281]: seen 705000 words at 3180.6 wps, loss = 3.941\n",
      "[batch 283]: seen 710000 words at 3179.9 wps, loss = 3.940\n",
      "[batch 285]: seen 715000 words at 3179.4 wps, loss = 3.940\n",
      "[batch 287]: seen 720000 words at 3179.1 wps, loss = 3.940\n",
      "[batch 289]: seen 725000 words at 3178.9 wps, loss = 3.939\n",
      "[batch 291]: seen 730000 words at 3178.8 wps, loss = 3.939\n",
      "[batch 293]: seen 735000 words at 3178.8 wps, loss = 3.939\n",
      "[batch 295]: seen 740000 words at 3178.8 wps, loss = 3.938\n",
      "[batch 297]: seen 745000 words at 3178.9 wps, loss = 3.938\n",
      "[batch 299]: seen 750000 words at 3178.8 wps, loss = 3.938\n",
      "[batch 301]: seen 755000 words at 3178.7 wps, loss = 3.938\n",
      "[batch 303]: seen 760000 words at 3179.1 wps, loss = 3.937\n",
      "[batch 305]: seen 765000 words at 3179.6 wps, loss = 3.937\n",
      "[batch 307]: seen 770000 words at 3180.2 wps, loss = 3.936\n",
      "[batch 309]: seen 775000 words at 3180.4 wps, loss = 3.936\n",
      "[batch 311]: seen 780000 words at 3180.8 wps, loss = 3.935\n",
      "[batch 313]: seen 785000 words at 3180.7 wps, loss = 3.936\n",
      "[batch 315]: seen 790000 words at 3181.1 wps, loss = 3.935\n",
      "[batch 317]: seen 795000 words at 3181.0 wps, loss = 3.935\n",
      "[batch 319]: seen 800000 words at 3181.3 wps, loss = 3.935\n",
      "[batch 321]: seen 805000 words at 3181.5 wps, loss = 3.935\n",
      "[batch 323]: seen 810000 words at 3181.9 wps, loss = 3.934\n",
      "[batch 325]: seen 815000 words at 3182.2 wps, loss = 3.934\n",
      "[batch 327]: seen 820000 words at 3182.1 wps, loss = 3.934\n",
      "[batch 329]: seen 825000 words at 3182.1 wps, loss = 3.933\n",
      "[batch 331]: seen 830000 words at 3182.0 wps, loss = 3.933\n",
      "[batch 333]: seen 835000 words at 3181.7 wps, loss = 3.933\n",
      "[batch 335]: seen 840000 words at 3181.2 wps, loss = 3.933\n",
      "[batch 337]: seen 845000 words at 3181.1 wps, loss = 3.932\n",
      "[batch 339]: seen 850000 words at 3181.1 wps, loss = 3.932\n",
      "[batch 341]: seen 855000 words at 3180.7 wps, loss = 3.932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 343]: seen 860000 words at 3179.2 wps, loss = 3.932\n",
      "[batch 345]: seen 865000 words at 3179.7 wps, loss = 3.932\n",
      "[batch 347]: seen 870000 words at 3180.0 wps, loss = 3.931\n",
      "[batch 349]: seen 875000 words at 3180.0 wps, loss = 3.931\n",
      "[batch 351]: seen 880000 words at 3180.3 wps, loss = 3.930\n",
      "[batch 353]: seen 885000 words at 3180.5 wps, loss = 3.930\n",
      "[batch 355]: seen 890000 words at 3180.7 wps, loss = 3.930\n",
      "[batch 357]: seen 895000 words at 3180.6 wps, loss = 3.930\n",
      "[batch 359]: seen 900000 words at 3180.9 wps, loss = 3.929\n",
      "[batch 361]: seen 905000 words at 3181.1 wps, loss = 3.929\n",
      "[batch 363]: seen 910000 words at 3181.5 wps, loss = 3.929\n",
      "[batch 365]: seen 915000 words at 3180.9 wps, loss = 3.928\n",
      "[batch 367]: seen 920000 words at 3180.4 wps, loss = 3.929\n",
      "[batch 369]: seen 925000 words at 3180.0 wps, loss = 3.928\n",
      "[batch 371]: seen 930000 words at 3179.5 wps, loss = 3.928\n",
      "[batch 373]: seen 935000 words at 3178.2 wps, loss = 3.928\n",
      "[batch 375]: seen 940000 words at 3177.5 wps, loss = 3.927\n",
      "[batch 377]: seen 945000 words at 3176.6 wps, loss = 3.927\n",
      "[batch 379]: seen 950000 words at 3175.7 wps, loss = 3.927\n",
      "[batch 381]: seen 955000 words at 3175.0 wps, loss = 3.927\n",
      "[batch 383]: seen 960000 words at 3174.9 wps, loss = 3.927\n",
      "[batch 385]: seen 965000 words at 3174.9 wps, loss = 3.927\n",
      "[batch 387]: seen 969900 words at 3174.7 wps, loss = 3.926\n",
      "[epoch 3] Completed in 0:05:05\n",
      "[epoch 4] Starting epoch 4\n",
      "[batch 1]: seen 5000 words at 3088.3 wps, loss = 5.158\n",
      "[batch 3]: seen 10000 words at 3121.1 wps, loss = 5.122\n",
      "[batch 5]: seen 15000 words at 3118.8 wps, loss = 5.098\n",
      "[batch 7]: seen 20000 words at 3098.5 wps, loss = 5.062\n",
      "[batch 9]: seen 25000 words at 3110.5 wps, loss = 5.034\n",
      "[batch 11]: seen 30000 words at 3114.2 wps, loss = 5.000\n",
      "[batch 13]: seen 35000 words at 3121.5 wps, loss = 4.954\n",
      "[batch 15]: seen 40000 words at 3130.5 wps, loss = 4.884\n",
      "[batch 17]: seen 45000 words at 3130.1 wps, loss = 4.785\n",
      "[batch 19]: seen 50000 words at 3127.7 wps, loss = 4.694\n",
      "[batch 21]: seen 55000 words at 3126.8 wps, loss = 4.624\n",
      "[batch 23]: seen 60000 words at 3125.9 wps, loss = 4.562\n",
      "[batch 25]: seen 65000 words at 3127.4 wps, loss = 4.514\n",
      "[batch 27]: seen 70000 words at 3126.0 wps, loss = 4.468\n",
      "[batch 29]: seen 75000 words at 3124.7 wps, loss = 4.429\n",
      "[batch 31]: seen 80000 words at 3125.6 wps, loss = 4.395\n",
      "[batch 33]: seen 85000 words at 3122.3 wps, loss = 4.371\n",
      "[batch 35]: seen 90000 words at 3122.0 wps, loss = 4.347\n",
      "[batch 37]: seen 95000 words at 3122.6 wps, loss = 4.323\n",
      "[batch 39]: seen 100000 words at 3123.9 wps, loss = 4.304\n",
      "[batch 41]: seen 105000 words at 3125.4 wps, loss = 4.282\n",
      "[batch 43]: seen 110000 words at 3125.8 wps, loss = 4.264\n",
      "[batch 45]: seen 115000 words at 3125.6 wps, loss = 4.248\n",
      "[batch 47]: seen 120000 words at 3126.0 wps, loss = 4.232\n",
      "[batch 49]: seen 125000 words at 3127.1 wps, loss = 4.218\n",
      "[batch 51]: seen 130000 words at 3123.6 wps, loss = 4.203\n",
      "[batch 53]: seen 135000 words at 3111.9 wps, loss = 4.191\n",
      "[batch 55]: seen 140000 words at 3100.8 wps, loss = 4.179\n",
      "[batch 57]: seen 145000 words at 3092.4 wps, loss = 4.168\n",
      "[batch 59]: seen 150000 words at 3083.4 wps, loss = 4.156\n",
      "[batch 61]: seen 155000 words at 3076.3 wps, loss = 4.146\n",
      "[batch 63]: seen 160000 words at 3073.3 wps, loss = 4.136\n",
      "[batch 65]: seen 165000 words at 3070.9 wps, loss = 4.128\n",
      "[batch 67]: seen 170000 words at 3068.7 wps, loss = 4.122\n",
      "[batch 69]: seen 175000 words at 3065.0 wps, loss = 4.115\n",
      "[batch 71]: seen 180000 words at 3065.8 wps, loss = 4.109\n",
      "[batch 73]: seen 185000 words at 3067.8 wps, loss = 4.103\n",
      "[batch 75]: seen 190000 words at 3069.3 wps, loss = 4.096\n",
      "[batch 77]: seen 195000 words at 3070.7 wps, loss = 4.090\n",
      "[batch 79]: seen 200000 words at 3070.4 wps, loss = 4.084\n",
      "[batch 81]: seen 205000 words at 3071.3 wps, loss = 4.079\n",
      "[batch 83]: seen 210000 words at 3072.8 wps, loss = 4.074\n",
      "[batch 85]: seen 215000 words at 3072.1 wps, loss = 4.068\n",
      "[batch 87]: seen 220000 words at 3072.6 wps, loss = 4.064\n",
      "[batch 89]: seen 225000 words at 3074.7 wps, loss = 4.060\n",
      "[batch 91]: seen 230000 words at 3076.2 wps, loss = 4.057\n",
      "[batch 93]: seen 235000 words at 3076.7 wps, loss = 4.052\n",
      "[batch 95]: seen 240000 words at 3076.2 wps, loss = 4.049\n",
      "[batch 97]: seen 245000 words at 3076.7 wps, loss = 4.045\n",
      "[batch 99]: seen 250000 words at 3076.4 wps, loss = 4.041\n",
      "[batch 101]: seen 255000 words at 3076.1 wps, loss = 4.037\n",
      "[batch 103]: seen 260000 words at 3075.2 wps, loss = 4.034\n",
      "[batch 105]: seen 265000 words at 3077.0 wps, loss = 4.030\n",
      "[batch 107]: seen 270000 words at 3077.7 wps, loss = 4.026\n",
      "[batch 109]: seen 275000 words at 3078.5 wps, loss = 4.023\n",
      "[batch 111]: seen 280000 words at 3079.0 wps, loss = 4.020\n",
      "[batch 113]: seen 285000 words at 3079.6 wps, loss = 4.017\n",
      "[batch 115]: seen 290000 words at 3082.0 wps, loss = 4.014\n",
      "[batch 117]: seen 295000 words at 3082.5 wps, loss = 4.011\n",
      "[batch 119]: seen 300000 words at 3083.2 wps, loss = 4.008\n",
      "[batch 121]: seen 305000 words at 3083.9 wps, loss = 4.007\n",
      "[batch 123]: seen 310000 words at 3084.8 wps, loss = 4.005\n",
      "[batch 125]: seen 315000 words at 3086.3 wps, loss = 4.004\n",
      "[batch 127]: seen 320000 words at 3087.7 wps, loss = 4.001\n",
      "[batch 129]: seen 325000 words at 3089.4 wps, loss = 3.999\n",
      "[batch 131]: seen 330000 words at 3091.1 wps, loss = 3.999\n",
      "[batch 133]: seen 335000 words at 3093.1 wps, loss = 3.996\n",
      "[batch 135]: seen 340000 words at 3093.7 wps, loss = 3.994\n",
      "[batch 137]: seen 345000 words at 3094.5 wps, loss = 3.992\n",
      "[batch 139]: seen 350000 words at 3094.8 wps, loss = 3.989\n",
      "[batch 141]: seen 355000 words at 3095.5 wps, loss = 3.991\n",
      "[batch 143]: seen 360000 words at 3096.5 wps, loss = 3.989\n",
      "[batch 145]: seen 365000 words at 3097.1 wps, loss = 3.989\n",
      "[batch 147]: seen 370000 words at 3097.7 wps, loss = 3.988\n",
      "[batch 149]: seen 375000 words at 3098.2 wps, loss = 3.985\n",
      "[batch 151]: seen 380000 words at 3098.8 wps, loss = 3.983\n",
      "[batch 153]: seen 385000 words at 3099.9 wps, loss = 3.981\n",
      "[batch 155]: seen 390000 words at 3100.6 wps, loss = 3.979\n",
      "[batch 157]: seen 395000 words at 3101.5 wps, loss = 3.977\n",
      "[batch 159]: seen 400000 words at 3101.4 wps, loss = 3.977\n",
      "[batch 161]: seen 405000 words at 3101.9 wps, loss = 3.975\n",
      "[batch 163]: seen 410000 words at 3102.7 wps, loss = 3.974\n",
      "[batch 165]: seen 415000 words at 3103.7 wps, loss = 3.972\n",
      "[batch 167]: seen 420000 words at 3103.9 wps, loss = 3.971\n",
      "[batch 169]: seen 425000 words at 3104.4 wps, loss = 3.970\n",
      "[batch 171]: seen 430000 words at 3105.5 wps, loss = 3.969\n",
      "[batch 173]: seen 435000 words at 3106.4 wps, loss = 3.967\n",
      "[batch 175]: seen 440000 words at 3106.8 wps, loss = 3.965\n",
      "[batch 177]: seen 445000 words at 3106.9 wps, loss = 3.964\n",
      "[batch 179]: seen 450000 words at 3107.3 wps, loss = 3.963\n",
      "[batch 181]: seen 455000 words at 3107.6 wps, loss = 3.962\n",
      "[batch 183]: seen 460000 words at 3108.1 wps, loss = 3.961\n",
      "[batch 185]: seen 465000 words at 3108.7 wps, loss = 3.960\n",
      "[batch 187]: seen 470000 words at 3109.2 wps, loss = 3.959\n",
      "[batch 189]: seen 475000 words at 3109.0 wps, loss = 3.958\n",
      "[batch 191]: seen 480000 words at 3108.9 wps, loss = 3.957\n",
      "[batch 193]: seen 485000 words at 3109.3 wps, loss = 3.955\n",
      "[batch 195]: seen 490000 words at 3110.0 wps, loss = 3.954\n",
      "[batch 197]: seen 495000 words at 3110.2 wps, loss = 3.953\n",
      "[batch 199]: seen 500000 words at 3109.8 wps, loss = 3.952\n",
      "[batch 201]: seen 505000 words at 3110.4 wps, loss = 3.951\n",
      "[batch 203]: seen 510000 words at 3111.0 wps, loss = 3.950\n",
      "[batch 205]: seen 515000 words at 3111.4 wps, loss = 3.949\n",
      "[batch 207]: seen 520000 words at 3111.8 wps, loss = 3.948\n",
      "[batch 209]: seen 525000 words at 3111.9 wps, loss = 3.947\n",
      "[batch 211]: seen 530000 words at 3112.3 wps, loss = 3.945\n",
      "[batch 213]: seen 535000 words at 3112.8 wps, loss = 3.945\n",
      "[batch 215]: seen 540000 words at 3113.7 wps, loss = 3.944\n",
      "[batch 217]: seen 545000 words at 3114.3 wps, loss = 3.942\n",
      "[batch 219]: seen 550000 words at 3114.8 wps, loss = 3.942\n",
      "[batch 221]: seen 555000 words at 3115.4 wps, loss = 3.941\n",
      "[batch 223]: seen 560000 words at 3116.1 wps, loss = 3.940\n",
      "[batch 225]: seen 565000 words at 3116.7 wps, loss = 3.939\n",
      "[batch 227]: seen 570000 words at 3117.4 wps, loss = 3.937\n",
      "[batch 229]: seen 575000 words at 3117.8 wps, loss = 3.936\n",
      "[batch 231]: seen 580000 words at 3118.2 wps, loss = 3.936\n",
      "[batch 233]: seen 585000 words at 3118.4 wps, loss = 3.935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 235]: seen 590000 words at 3118.8 wps, loss = 3.934\n",
      "[batch 237]: seen 595000 words at 3119.2 wps, loss = 3.934\n",
      "[batch 239]: seen 600000 words at 3119.8 wps, loss = 3.933\n",
      "[batch 241]: seen 605000 words at 3119.8 wps, loss = 3.932\n",
      "[batch 243]: seen 610000 words at 3119.8 wps, loss = 3.931\n",
      "[batch 245]: seen 615000 words at 3119.5 wps, loss = 3.930\n",
      "[batch 247]: seen 620000 words at 3119.9 wps, loss = 3.929\n",
      "[batch 249]: seen 625000 words at 3120.0 wps, loss = 3.928\n",
      "[batch 251]: seen 630000 words at 3120.3 wps, loss = 3.927\n",
      "[batch 253]: seen 635000 words at 3120.4 wps, loss = 3.926\n",
      "[batch 255]: seen 640000 words at 3119.3 wps, loss = 3.925\n",
      "[batch 257]: seen 645000 words at 3119.5 wps, loss = 3.924\n",
      "[batch 259]: seen 650000 words at 3119.5 wps, loss = 3.924\n",
      "[batch 261]: seen 655000 words at 3119.2 wps, loss = 3.923\n",
      "[batch 263]: seen 660000 words at 3118.9 wps, loss = 3.923\n",
      "[batch 265]: seen 665000 words at 3118.9 wps, loss = 3.922\n",
      "[batch 267]: seen 670000 words at 3118.9 wps, loss = 3.921\n",
      "[batch 269]: seen 675000 words at 3119.0 wps, loss = 3.921\n",
      "[batch 271]: seen 680000 words at 3119.1 wps, loss = 3.920\n",
      "[batch 273]: seen 685000 words at 3119.0 wps, loss = 3.919\n",
      "[batch 275]: seen 690000 words at 3119.5 wps, loss = 3.919\n",
      "[batch 277]: seen 695000 words at 3119.8 wps, loss = 3.918\n",
      "[batch 279]: seen 700000 words at 3120.0 wps, loss = 3.917\n",
      "[batch 281]: seen 705000 words at 3120.3 wps, loss = 3.917\n",
      "[batch 283]: seen 710000 words at 3120.3 wps, loss = 3.916\n",
      "[batch 285]: seen 715000 words at 3119.7 wps, loss = 3.916\n",
      "[batch 287]: seen 720000 words at 3119.1 wps, loss = 3.915\n",
      "[batch 289]: seen 725000 words at 3118.7 wps, loss = 3.915\n",
      "[batch 291]: seen 730000 words at 3118.5 wps, loss = 3.915\n",
      "[batch 293]: seen 735000 words at 3118.6 wps, loss = 3.914\n",
      "[batch 295]: seen 740000 words at 3119.1 wps, loss = 3.913\n",
      "[batch 297]: seen 745000 words at 3119.5 wps, loss = 3.913\n",
      "[batch 299]: seen 750000 words at 3119.6 wps, loss = 3.913\n",
      "[batch 301]: seen 755000 words at 3119.5 wps, loss = 3.912\n",
      "[batch 303]: seen 760000 words at 3119.5 wps, loss = 3.911\n",
      "[batch 305]: seen 765000 words at 3118.9 wps, loss = 3.910\n",
      "[batch 307]: seen 770000 words at 3118.5 wps, loss = 3.910\n",
      "[batch 309]: seen 775000 words at 3118.8 wps, loss = 3.909\n",
      "[batch 311]: seen 780000 words at 3119.1 wps, loss = 3.908\n",
      "[batch 313]: seen 785000 words at 3119.3 wps, loss = 3.908\n",
      "[batch 315]: seen 790000 words at 3119.8 wps, loss = 3.907\n",
      "[batch 317]: seen 795000 words at 3119.9 wps, loss = 3.907\n",
      "[batch 319]: seen 800000 words at 3120.0 wps, loss = 3.907\n",
      "[batch 321]: seen 805000 words at 3120.5 wps, loss = 3.906\n",
      "[batch 323]: seen 810000 words at 3120.8 wps, loss = 3.905\n",
      "[batch 325]: seen 815000 words at 3121.0 wps, loss = 3.905\n",
      "[batch 327]: seen 820000 words at 3120.7 wps, loss = 3.905\n",
      "[batch 329]: seen 825000 words at 3120.4 wps, loss = 3.904\n",
      "[batch 331]: seen 830000 words at 3120.7 wps, loss = 3.903\n",
      "[batch 333]: seen 835000 words at 3121.1 wps, loss = 3.903\n",
      "[batch 335]: seen 840000 words at 3121.5 wps, loss = 3.902\n",
      "[batch 337]: seen 845000 words at 3121.6 wps, loss = 3.902\n",
      "[batch 339]: seen 850000 words at 3121.6 wps, loss = 3.901\n",
      "[batch 341]: seen 855000 words at 3122.0 wps, loss = 3.901\n",
      "[batch 343]: seen 860000 words at 3122.2 wps, loss = 3.901\n",
      "[batch 345]: seen 865000 words at 3122.6 wps, loss = 3.900\n",
      "[batch 347]: seen 870000 words at 3122.9 wps, loss = 3.900\n",
      "[batch 349]: seen 875000 words at 3122.7 wps, loss = 3.900\n",
      "[batch 351]: seen 880000 words at 3122.6 wps, loss = 3.899\n",
      "[batch 353]: seen 885000 words at 3122.5 wps, loss = 3.898\n",
      "[batch 355]: seen 890000 words at 3122.7 wps, loss = 3.898\n",
      "[batch 357]: seen 895000 words at 3123.0 wps, loss = 3.897\n",
      "[batch 359]: seen 900000 words at 3123.4 wps, loss = 3.897\n",
      "[batch 361]: seen 905000 words at 3123.3 wps, loss = 3.896\n",
      "[batch 363]: seen 910000 words at 3121.6 wps, loss = 3.896\n",
      "[batch 365]: seen 915000 words at 3121.5 wps, loss = 3.895\n",
      "[batch 367]: seen 920000 words at 3121.8 wps, loss = 3.895\n",
      "[batch 369]: seen 925000 words at 3122.0 wps, loss = 3.895\n",
      "[batch 371]: seen 930000 words at 3121.9 wps, loss = 3.895\n",
      "[batch 373]: seen 935000 words at 3122.0 wps, loss = 3.894\n",
      "[batch 375]: seen 940000 words at 3121.9 wps, loss = 3.894\n",
      "[batch 377]: seen 945000 words at 3121.7 wps, loss = 3.893\n",
      "[batch 379]: seen 950000 words at 3121.8 wps, loss = 3.893\n",
      "[batch 381]: seen 955000 words at 3121.7 wps, loss = 3.893\n",
      "[batch 383]: seen 960000 words at 3121.9 wps, loss = 3.893\n",
      "[batch 385]: seen 965000 words at 3122.0 wps, loss = 3.893\n",
      "[batch 387]: seen 969900 words at 3122.1 wps, loss = 3.892\n",
      "[epoch 4] Completed in 0:05:10\n",
      "[epoch 5] Starting epoch 5\n",
      "[batch 1]: seen 5000 words at 3003.5 wps, loss = 3.817\n",
      "[batch 3]: seen 10000 words at 3058.6 wps, loss = 3.808\n",
      "[batch 5]: seen 15000 words at 3043.3 wps, loss = 3.804\n",
      "[batch 7]: seen 20000 words at 3082.8 wps, loss = 3.800\n",
      "[batch 9]: seen 25000 words at 3097.9 wps, loss = 3.805\n",
      "[batch 11]: seen 30000 words at 3089.3 wps, loss = 3.813\n",
      "[batch 13]: seen 35000 words at 3092.9 wps, loss = 3.819\n",
      "[batch 15]: seen 40000 words at 3093.1 wps, loss = 3.818\n",
      "[batch 17]: seen 45000 words at 3081.6 wps, loss = 3.825\n",
      "[batch 19]: seen 50000 words at 3096.5 wps, loss = 3.822\n",
      "[batch 21]: seen 55000 words at 3112.9 wps, loss = 3.823\n",
      "[batch 23]: seen 60000 words at 3121.6 wps, loss = 3.825\n",
      "[batch 25]: seen 65000 words at 3128.6 wps, loss = 3.819\n",
      "[batch 27]: seen 70000 words at 3136.9 wps, loss = 3.818\n",
      "[batch 29]: seen 75000 words at 3131.5 wps, loss = 3.818\n",
      "[batch 31]: seen 80000 words at 3127.2 wps, loss = 3.817\n",
      "[batch 33]: seen 85000 words at 3126.4 wps, loss = 3.816\n",
      "[batch 35]: seen 90000 words at 3126.1 wps, loss = 3.819\n",
      "[batch 37]: seen 95000 words at 3122.0 wps, loss = 3.818\n",
      "[batch 39]: seen 100000 words at 3117.9 wps, loss = 3.819\n",
      "[batch 41]: seen 105000 words at 3115.2 wps, loss = 3.817\n",
      "[batch 43]: seen 110000 words at 3114.6 wps, loss = 3.815\n",
      "[batch 45]: seen 115000 words at 3116.0 wps, loss = 3.817\n",
      "[batch 47]: seen 120000 words at 3120.3 wps, loss = 3.815\n",
      "[batch 49]: seen 125000 words at 3124.0 wps, loss = 3.825\n",
      "[batch 51]: seen 130000 words at 3122.7 wps, loss = 3.828\n",
      "[batch 53]: seen 135000 words at 3112.2 wps, loss = 3.825\n",
      "[batch 55]: seen 140000 words at 3103.8 wps, loss = 3.825\n",
      "[batch 57]: seen 145000 words at 3092.7 wps, loss = 3.825\n",
      "[batch 59]: seen 150000 words at 3083.1 wps, loss = 3.824\n",
      "[batch 61]: seen 155000 words at 3075.9 wps, loss = 3.823\n",
      "[batch 63]: seen 160000 words at 3074.0 wps, loss = 3.821\n",
      "[batch 65]: seen 165000 words at 3072.9 wps, loss = 3.822\n",
      "[batch 67]: seen 170000 words at 3071.7 wps, loss = 3.820\n",
      "[batch 69]: seen 175000 words at 3072.3 wps, loss = 3.818\n",
      "[batch 71]: seen 180000 words at 3074.1 wps, loss = 3.817\n",
      "[batch 73]: seen 185000 words at 3076.1 wps, loss = 3.818\n",
      "[batch 75]: seen 190000 words at 3078.2 wps, loss = 3.818\n",
      "[batch 77]: seen 195000 words at 3081.8 wps, loss = 3.818\n",
      "[batch 79]: seen 200000 words at 3083.7 wps, loss = 3.817\n",
      "[batch 81]: seen 205000 words at 3084.8 wps, loss = 3.822\n",
      "[batch 83]: seen 210000 words at 3086.5 wps, loss = 3.822\n",
      "[batch 85]: seen 215000 words at 3088.0 wps, loss = 3.821\n",
      "[batch 87]: seen 220000 words at 3089.7 wps, loss = 3.821\n",
      "[batch 89]: seen 225000 words at 3090.6 wps, loss = 3.820\n",
      "[batch 91]: seen 230000 words at 3092.1 wps, loss = 3.819\n",
      "[batch 93]: seen 235000 words at 3093.3 wps, loss = 3.818\n",
      "[batch 95]: seen 240000 words at 3094.0 wps, loss = 3.818\n",
      "[batch 97]: seen 245000 words at 3094.9 wps, loss = 3.817\n",
      "[batch 99]: seen 250000 words at 3095.3 wps, loss = 3.817\n",
      "[batch 101]: seen 255000 words at 3095.9 wps, loss = 3.816\n",
      "[batch 103]: seen 260000 words at 3095.8 wps, loss = 3.816\n",
      "[batch 105]: seen 265000 words at 3096.7 wps, loss = 3.815\n",
      "[batch 107]: seen 270000 words at 3098.1 wps, loss = 3.813\n",
      "[batch 109]: seen 275000 words at 3099.3 wps, loss = 3.814\n",
      "[batch 111]: seen 280000 words at 3099.9 wps, loss = 3.815\n",
      "[batch 113]: seen 285000 words at 3100.0 wps, loss = 3.814\n",
      "[batch 115]: seen 290000 words at 3100.9 wps, loss = 3.814\n",
      "[batch 117]: seen 295000 words at 3101.3 wps, loss = 3.814\n",
      "[batch 119]: seen 300000 words at 3102.3 wps, loss = 3.813\n",
      "[batch 121]: seen 305000 words at 3103.3 wps, loss = 3.813\n",
      "[batch 123]: seen 310000 words at 3103.7 wps, loss = 3.814\n",
      "[batch 125]: seen 315000 words at 3103.2 wps, loss = 3.814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 127]: seen 320000 words at 3103.0 wps, loss = 3.814\n",
      "[batch 129]: seen 325000 words at 3103.8 wps, loss = 3.814\n",
      "[batch 131]: seen 330000 words at 3104.1 wps, loss = 3.813\n",
      "[batch 133]: seen 335000 words at 3103.2 wps, loss = 3.813\n",
      "[batch 135]: seen 340000 words at 3102.7 wps, loss = 3.813\n",
      "[batch 137]: seen 345000 words at 3102.9 wps, loss = 3.813\n",
      "[batch 139]: seen 350000 words at 3103.0 wps, loss = 3.812\n",
      "[batch 141]: seen 355000 words at 3103.1 wps, loss = 3.811\n",
      "[batch 143]: seen 360000 words at 3103.1 wps, loss = 3.812\n",
      "[batch 145]: seen 365000 words at 3103.0 wps, loss = 3.811\n",
      "[batch 147]: seen 370000 words at 3102.3 wps, loss = 3.811\n",
      "[batch 149]: seen 375000 words at 3102.2 wps, loss = 3.810\n",
      "[batch 151]: seen 380000 words at 3102.4 wps, loss = 3.810\n",
      "[batch 153]: seen 385000 words at 3102.8 wps, loss = 3.809\n",
      "[batch 155]: seen 390000 words at 3102.4 wps, loss = 3.808\n",
      "[batch 157]: seen 395000 words at 3102.5 wps, loss = 3.807\n",
      "[batch 159]: seen 400000 words at 3102.4 wps, loss = 3.807\n",
      "[batch 161]: seen 405000 words at 3102.5 wps, loss = 3.808\n",
      "[batch 163]: seen 410000 words at 3102.8 wps, loss = 3.808\n",
      "[batch 165]: seen 415000 words at 3100.5 wps, loss = 3.807\n",
      "[batch 167]: seen 420000 words at 3100.2 wps, loss = 3.808\n",
      "[batch 169]: seen 425000 words at 3099.9 wps, loss = 3.808\n",
      "[batch 171]: seen 430000 words at 3100.1 wps, loss = 3.808\n",
      "[batch 173]: seen 435000 words at 3100.6 wps, loss = 3.807\n",
      "[batch 175]: seen 440000 words at 3100.8 wps, loss = 3.806\n",
      "[batch 177]: seen 445000 words at 3101.1 wps, loss = 3.806\n",
      "[batch 179]: seen 450000 words at 3100.7 wps, loss = 3.806\n",
      "[batch 181]: seen 455000 words at 3100.2 wps, loss = 3.806\n",
      "[batch 183]: seen 460000 words at 3100.2 wps, loss = 3.805\n",
      "[batch 185]: seen 465000 words at 3100.4 wps, loss = 3.805\n",
      "[batch 187]: seen 470000 words at 3100.5 wps, loss = 3.805\n",
      "[batch 189]: seen 475000 words at 3100.9 wps, loss = 3.805\n",
      "[batch 191]: seen 480000 words at 3100.8 wps, loss = 3.805\n",
      "[batch 193]: seen 485000 words at 3101.0 wps, loss = 3.805\n",
      "[batch 195]: seen 490000 words at 3101.2 wps, loss = 3.804\n",
      "[batch 197]: seen 495000 words at 3102.5 wps, loss = 3.804\n",
      "[batch 199]: seen 500000 words at 3103.7 wps, loss = 3.803\n",
      "[batch 201]: seen 505000 words at 3103.6 wps, loss = 3.803\n",
      "[batch 203]: seen 510000 words at 3104.0 wps, loss = 3.802\n",
      "[batch 205]: seen 515000 words at 3104.3 wps, loss = 3.802\n",
      "[batch 207]: seen 520000 words at 3104.5 wps, loss = 3.801\n",
      "[batch 209]: seen 525000 words at 3104.9 wps, loss = 3.801\n",
      "[batch 211]: seen 530000 words at 3104.9 wps, loss = 3.801\n",
      "[batch 213]: seen 535000 words at 3104.9 wps, loss = 3.801\n",
      "[batch 215]: seen 540000 words at 3105.0 wps, loss = 3.801\n",
      "[batch 217]: seen 545000 words at 3105.0 wps, loss = 3.800\n",
      "[batch 219]: seen 550000 words at 3105.0 wps, loss = 3.801\n",
      "[batch 221]: seen 555000 words at 3105.2 wps, loss = 3.801\n",
      "[batch 223]: seen 560000 words at 3105.1 wps, loss = 3.800\n",
      "[batch 225]: seen 565000 words at 3104.6 wps, loss = 3.800\n",
      "[batch 227]: seen 570000 words at 3104.7 wps, loss = 3.800\n",
      "[batch 229]: seen 575000 words at 3104.8 wps, loss = 3.800\n",
      "[batch 231]: seen 580000 words at 3104.5 wps, loss = 3.799\n",
      "[batch 233]: seen 585000 words at 3104.6 wps, loss = 3.800\n",
      "[batch 235]: seen 590000 words at 3104.4 wps, loss = 3.799\n",
      "[batch 237]: seen 595000 words at 3104.3 wps, loss = 3.799\n",
      "[batch 239]: seen 600000 words at 3104.3 wps, loss = 3.799\n",
      "[batch 241]: seen 605000 words at 3103.8 wps, loss = 3.799\n",
      "[batch 243]: seen 610000 words at 3103.8 wps, loss = 3.798\n",
      "[batch 245]: seen 615000 words at 3104.2 wps, loss = 3.798\n",
      "[batch 247]: seen 620000 words at 3105.5 wps, loss = 3.799\n",
      "[batch 249]: seen 625000 words at 3106.7 wps, loss = 3.798\n",
      "[batch 251]: seen 630000 words at 3107.9 wps, loss = 3.798\n",
      "[batch 253]: seen 635000 words at 3107.8 wps, loss = 3.798\n",
      "[batch 255]: seen 640000 words at 3107.6 wps, loss = 3.798\n",
      "[batch 257]: seen 645000 words at 3107.7 wps, loss = 3.798\n",
      "[batch 259]: seen 650000 words at 3107.7 wps, loss = 3.798\n",
      "[batch 261]: seen 655000 words at 3108.1 wps, loss = 3.798\n",
      "[batch 263]: seen 660000 words at 3107.9 wps, loss = 3.798\n",
      "[batch 265]: seen 665000 words at 3107.8 wps, loss = 3.797\n",
      "[batch 267]: seen 670000 words at 3107.8 wps, loss = 3.798\n",
      "[batch 269]: seen 675000 words at 3108.1 wps, loss = 3.798\n",
      "[batch 271]: seen 680000 words at 3108.4 wps, loss = 3.798\n",
      "[batch 273]: seen 685000 words at 3108.9 wps, loss = 3.797\n",
      "[batch 275]: seen 690000 words at 3109.2 wps, loss = 3.797\n",
      "[batch 277]: seen 695000 words at 3109.8 wps, loss = 3.797\n",
      "[batch 279]: seen 700000 words at 3110.9 wps, loss = 3.797\n",
      "[batch 281]: seen 705000 words at 3111.0 wps, loss = 3.797\n",
      "[batch 283]: seen 710000 words at 3110.5 wps, loss = 3.797\n",
      "[batch 285]: seen 715000 words at 3110.5 wps, loss = 3.797\n",
      "[batch 287]: seen 720000 words at 3110.8 wps, loss = 3.797\n",
      "[batch 289]: seen 725000 words at 3110.5 wps, loss = 3.797\n",
      "[batch 291]: seen 730000 words at 3110.3 wps, loss = 3.798\n",
      "[batch 293]: seen 735000 words at 3110.1 wps, loss = 3.797\n",
      "[batch 295]: seen 740000 words at 3109.9 wps, loss = 3.797\n",
      "[batch 297]: seen 745000 words at 3110.0 wps, loss = 3.797\n",
      "[batch 299]: seen 750000 words at 3109.9 wps, loss = 3.797\n",
      "[batch 301]: seen 755000 words at 3110.3 wps, loss = 3.797\n",
      "[batch 303]: seen 760000 words at 3110.7 wps, loss = 3.797\n",
      "[batch 305]: seen 765000 words at 3110.9 wps, loss = 3.797\n",
      "[batch 307]: seen 770000 words at 3111.5 wps, loss = 3.796\n",
      "[batch 309]: seen 775000 words at 3112.0 wps, loss = 3.796\n",
      "[batch 311]: seen 780000 words at 3112.3 wps, loss = 3.796\n",
      "[batch 313]: seen 785000 words at 3112.4 wps, loss = 3.796\n",
      "[batch 315]: seen 790000 words at 3112.0 wps, loss = 3.796\n",
      "[batch 317]: seen 795000 words at 3112.3 wps, loss = 3.796\n",
      "[batch 319]: seen 800000 words at 3112.3 wps, loss = 3.796\n",
      "[batch 321]: seen 805000 words at 3112.2 wps, loss = 3.796\n",
      "[batch 323]: seen 810000 words at 3112.4 wps, loss = 3.795\n",
      "[batch 325]: seen 815000 words at 3112.5 wps, loss = 3.795\n",
      "[batch 327]: seen 820000 words at 3112.5 wps, loss = 3.795\n",
      "[batch 329]: seen 825000 words at 3112.6 wps, loss = 3.795\n",
      "[batch 331]: seen 830000 words at 3112.5 wps, loss = 3.795\n",
      "[batch 333]: seen 835000 words at 3112.5 wps, loss = 3.795\n",
      "[batch 335]: seen 840000 words at 3112.7 wps, loss = 3.795\n",
      "[batch 337]: seen 845000 words at 3112.9 wps, loss = 3.795\n",
      "[batch 339]: seen 850000 words at 3113.0 wps, loss = 3.794\n",
      "[batch 341]: seen 855000 words at 3112.9 wps, loss = 3.794\n",
      "[batch 343]: seen 860000 words at 3112.8 wps, loss = 3.794\n",
      "[batch 345]: seen 865000 words at 3112.9 wps, loss = 3.794\n",
      "[batch 347]: seen 870000 words at 3112.8 wps, loss = 3.794\n",
      "[batch 349]: seen 875000 words at 3112.6 wps, loss = 3.794\n",
      "[batch 351]: seen 880000 words at 3112.1 wps, loss = 3.793\n",
      "[batch 353]: seen 885000 words at 3111.8 wps, loss = 3.794\n",
      "[batch 355]: seen 890000 words at 3111.7 wps, loss = 3.794\n",
      "[batch 357]: seen 895000 words at 3111.7 wps, loss = 3.793\n",
      "[batch 359]: seen 900000 words at 3111.2 wps, loss = 3.793\n",
      "[batch 361]: seen 905000 words at 3111.1 wps, loss = 3.793\n",
      "[batch 363]: seen 910000 words at 3110.8 wps, loss = 3.794\n",
      "[batch 365]: seen 915000 words at 3110.6 wps, loss = 3.793\n",
      "[batch 367]: seen 920000 words at 3110.7 wps, loss = 3.794\n",
      "[batch 369]: seen 925000 words at 3111.0 wps, loss = 3.794\n",
      "[batch 371]: seen 930000 words at 3111.2 wps, loss = 3.794\n",
      "[batch 373]: seen 935000 words at 3110.9 wps, loss = 3.794\n",
      "[batch 375]: seen 940000 words at 3110.8 wps, loss = 3.794\n",
      "[batch 377]: seen 945000 words at 3110.8 wps, loss = 3.793\n",
      "[batch 379]: seen 950000 words at 3110.9 wps, loss = 3.793\n",
      "[batch 381]: seen 955000 words at 3111.0 wps, loss = 3.793\n",
      "[batch 383]: seen 960000 words at 3110.9 wps, loss = 3.793\n",
      "[batch 385]: seen 965000 words at 3110.9 wps, loss = 3.793\n",
      "[batch 387]: seen 969900 words at 3110.6 wps, loss = 3.793\n",
      "[epoch 5] Completed in 0:05:11\n",
      "[epoch 5] Train set: avg. loss: 4.972  (perplexity: 144.30)\n",
      "[epoch 5] Test set: avg. loss: 5.066  (perplexity: 158.47)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "    \n",
    "    \n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = utils.rnnlm_batch_generator(train_ids, batch_size, max_time)\n",
    "        print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Run a training epoch.\n",
    "        run_epoch(lm, session, bi,True,\n",
    "                  True,1.0,learning_rate)\n",
    "        \n",
    "        #### END(YOUR CODE) ####\n",
    "        print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpointls \n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "      #  print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "      #  score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "      #  print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "      #  score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "      #  print(\"\")\n",
    "    print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "    score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "    print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "    score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "    print(\"\")\n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Sampling Sentences (5 points)\n",
    "\n",
    "If you didn't already in **part (b)**, implement the `BuildSamplerGraph()` method in `rnnlm.py` See the function docstring for more information.\n",
    "\n",
    "#### Implement the `sample_step()` method below (5 points)\n",
    "This should access the Tensors you create in `BuildSamplerGraph()`. Given an input batch and initial states, it should return a vector of shape `[batch_size,1]` containing sampled indices for the next word of each batch sequence.\n",
    "\n",
    "Run the method using the provided code to generate 10 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_step(lm, session, input_w, initial_h):\n",
    "    \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "    Args:\n",
    "      lm : rnnlm.RNNLM\n",
    "      session: tf.Session\n",
    "      input_w : [batch_size] vector of indices\n",
    "      initial_h : [batch_size, hidden_dims] initial state\n",
    "    \n",
    "    Returns:\n",
    "      final_h : final hidden state, compatible with initial_h\n",
    "      samples : [batch_size, 1] vector of indices\n",
    "    \"\"\"\n",
    "    # Reshape input to column vector\n",
    "    input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "    \n",
    "    #### YOUR CODE HERE ####\n",
    "    # Run sample ops\n",
    "\n",
    "    feed_dict = {lm.input_w_:input_w,\n",
    "                 lm.initial_h_:initial_h}\n",
    "    ops = [lm.pred_samples_, lm.final_h_]        \n",
    "\n",
    "    samples,final_h = session.run(ops,feed_dict=feed_dict)    \n",
    "\n",
    "    #### END(YOUR CODE) ####\n",
    "    # Note indexing here: \n",
    "    #   [batch_size, max_time, 1] -> [batch_size, 1]\n",
    "    return final_h, samples[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_1:0\", shape=(?, ?, 1), dtype=int64)\n",
      "INFO:tensorflow:Restoring parameters from /tmp/w266/a3_model/rnnlm_trained\n",
      "<s> mantle <unk> one , for the <unk> catholic <unk> skyros with some of the battery during the land . <s> \n",
      "<s> this evidence ) develops the artist and token <unk> typical aid inquirer . <s> \n",
      "<s> don't congregational . <s> \n",
      "<s> it <unk> way to the space to check more to the rest of cause , had picturesque orchestra friday familiar \n",
      "<s> his , part-time and <unk> job in writing or atmosphere . <s> \n",
      "<s> he may butyrate a <unk> cheek of the public of the antique edge that a way of the same opposition \n",
      "<s> yes , should close himself down and went out down him . <s> \n",
      "<s> there were no day l . <s> \n",
      "<s> the <unk> that were greatly supplementary to miss experience , and they may make that i have often been trouble \n",
      "<s> we need financial and do for us . <s> \n"
     ]
    }
   ],
   "source": [
    "# Same as above, but as a batch\n",
    "max_steps = 20\n",
    "num_samples = 10\n",
    "random_seed = 42\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "reload(rnnlm)\n",
    "\n",
    "with lm.graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(random_seed)\n",
    "    \n",
    "    # Load the trained model\n",
    "    saver.restore(session, trained_filename)\n",
    "\n",
    "    # Make initial state for a batch with batch_size = num_samples\n",
    "    w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    # We'll take one step for each sequence on each iteration \n",
    "    for i in range(max_steps):\n",
    "        h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "        w = np.hstack((w,y))\n",
    "\n",
    "    # Print generated sentences\n",
    "    for row in w:\n",
    "        for i, word_id in enumerate(row):\n",
    "            print(vocab.id_to_word[word_id], end=\" \")\n",
    "            if (i != 0) and (word_id == vocab.START_ID):\n",
    "                break\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## (e) Linguistic Properties (5 points)\n",
    "\n",
    "Now that we've trained our RNNLM, let's test a few properties of the model to see how well it learns linguistic phenomena. We'll do this with a scoring task: given two or more test sentences, our model should score the more plausible (or more correct) sentence with a higher log-probability.\n",
    "\n",
    "We'll define a scoring function to help us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_seq(lm, session, seq, vocab):\n",
    "    \"\"\"Score a sequence of words. Returns total log-probability.\"\"\"\n",
    "    padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq + [\"</s>\"], \n",
    "                                                             wordset=vocab.word_to_id))\n",
    "    w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "    y = np.reshape(padded_ids[1:],  [1,-1])\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    feed_dict = {lm.input_w_:w,\n",
    "                 lm.target_y_:y,\n",
    "                 lm.initial_h_:h,\n",
    "                 lm.dropout_keep_prob_: 1.0}\n",
    "    # Return log(P(seq)) = -1*loss\n",
    "    return -1*session.run(lm.loss_, feed_dict)\n",
    "\n",
    "def load_and_score(inputs, sort=False):\n",
    "    \"\"\"Load the trained model and score the given words.\"\"\"\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=lm.graph) as session:  \n",
    "        # Load the trained model\n",
    "        saver.restore(session, trained_filename)\n",
    "\n",
    "        if isinstance(inputs[0], str) or isinstance(inputs[0], bytes):\n",
    "            inputs = [inputs]\n",
    "\n",
    "        # Actually run scoring\n",
    "        results = []\n",
    "        for words in inputs:\n",
    "            score = score_seq(lm, session, words, vocab)\n",
    "            results.append((score, words))\n",
    "\n",
    "        # Sort if requested\n",
    "        if sort: results = sorted(results, reverse=True)\n",
    "\n",
    "        # Print results\n",
    "        for score, words in results:\n",
    "            print(\"\\\"{:s}\\\" : {:.02f}\".format(\" \".join(words), score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/w266/a3_model/rnnlm_trained\n",
      "\"once upon a time\" : -7.49\n",
      "\"the quick brown fox jumps over the lazy dog\" : -7.23\n"
     ]
    }
   ],
   "source": [
    "sents = [\"once upon a time\",\n",
    "         \"the quick brown fox jumps over the lazy dog\"]\n",
    "load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Number agreement\n",
    "\n",
    "Compare **\"the boy and the girl [are/is]\"**. Which is more plausible according to your model?\n",
    "\n",
    "If your model doesn't order them correctly (*this is OK*), why do you think that might be? (answer in cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/w266/a3_model/rnnlm_trained\n",
      "\"the boy and the girl are\" : -5.56\n",
      "\"the boy and the girl is\" : -5.47\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "\n",
    "sents = [\"the boy and the girl are\",\n",
    "         \"the boy and the girl is\"]\n",
    "load_and_score([s.split() for s in sents])\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Type/semantic agreement\n",
    "\n",
    "Compare:\n",
    "- **\"peanuts are my favorite kind of [nut/vegetable]\"**\n",
    "- **\"when I'm hungry I really prefer to [eat/drink]\"**\n",
    "\n",
    "Of each pair, which is more plausible according to your model?\n",
    "\n",
    "How would you expect a 3-gram language model to perform at this example? How about a 5-gram model? (answer in cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/w266/a3_model/rnnlm_trained\n",
      "\"peanuts are my favorite kind of nut\" : -6.87\n",
      "\"peanuts are my favorite kind of vegetable\" : -6.63\n",
      "\"when I'm hungry I really prefer to eat\" : -7.17\n",
      "\"when I'm hungry I really prefer to drink\" : -7.26\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "\n",
    "\n",
    "sents = [\"peanuts are my favorite kind of nut\",\n",
    "         \"peanuts are my favorite kind of vegetable\",\n",
    "         \"when I'm hungry I really prefer to eat\",\n",
    "         \"when I'm hungry I really prefer to drink\"]\n",
    "load_and_score([s.split() for s in sents])\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Adjective ordering (just for fun)\n",
    "\n",
    "Let's repeat the exercise from Week 2:\n",
    "\n",
    "![Adjective Order](adjective_order.jpg)\n",
    "*source: https://twitter.com/MattAndersonBBC/status/772002757222002688?lang=en*\n",
    "\n",
    "We'll consider a toy example (literally), and consider all possible adjective permutations.\n",
    "\n",
    "Note that this is somewhat sensitive to training, and even a good language model might not get it all correct. Why might the NN fail, if the trigram model from Week 2 was able to solve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/w266/a3_model/rnnlm_trained\n",
      "\"I have lots of plastic green square toys\" : -7.73\n",
      "\"I have lots of green square plastic toys\" : -7.73\n",
      "\"I have lots of green plastic square toys\" : -7.74\n",
      "\"I have lots of plastic square green toys\" : -7.76\n",
      "\"I have lots of square green plastic toys\" : -7.85\n",
      "\"I have lots of square plastic green toys\" : -7.92\n"
     ]
    }
   ],
   "source": [
    "prefix = \"I have lots of\".split()\n",
    "noun = \"toys\"\n",
    "adjectives = [\"square\", \"green\", \"plastic\"]\n",
    "inputs = []\n",
    "for adjs in itertools.permutations(adjectives):\n",
    "    words = prefix + list(adjs) + [noun]\n",
    "    inputs.append(words)\n",
    "    \n",
    "load_and_score(inputs, sort=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
